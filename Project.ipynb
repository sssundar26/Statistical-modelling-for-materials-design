{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import uniform \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data matrix generation & preprocessing (removing highly correlated features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y matrix generation\n",
    "y_vec=pd.read_csv(\"aqsol data.txt\",header=None)\n",
    "Y=y_vec[1] #Target matrix ()\n",
    "Y=np.array(Y)\n",
    "x_vec=pd.read_csv(\"file1.csv\",index_col=0)\n",
    "X=x_vec.values\n",
    "X=X[:,:-1] #feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of generated signature descriptors 3379\n"
     ]
    }
   ],
   "source": [
    "print(\"total no of generated signature descriptors\",len(x_vec.columns))\n",
    "new_vec=x_vec.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove features with more than 90 percent correlation\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    del dataset[colname] # deleting the column from the dataset\n",
    "\n",
    "    return(dataset)\n",
    "uncor=correlation(new_vec,0.9)\n",
    "X=uncor.values\n",
    "X=X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of signature descriptors after removing feature with more than 90% correlation 1571\n"
     ]
    }
   ],
   "source": [
    "print(\"total no of signature descriptors after removing feature with more than 90% correlation\",len(uncor.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of the aqsol data:\n",
      "Maximum:  1.58\n",
      "Minimum:  -11.6\n",
      "Average: -3.0587655293088365\n"
     ]
    }
   ],
   "source": [
    "# Exploratory Data analysis \n",
    "print (\"Statistics of the aqsol data:\")\n",
    "maxElement = np.amax(Y)\n",
    "print('Maximum: ', maxElement)\n",
    "minElement = np.amin(Y)\n",
    "print('Minimum: ', minElement)\n",
    "print ('Average:',np.average(Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler= MinMaxScaler()\n",
    "X_scaled=Scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 feature selection (Hyperparameter tuning using grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0042243686391818525\n",
      "0.8037845591191454\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': uniform()}\n",
    "model = Lasso()\n",
    "rand_search = RandomizedSearchCV(estimator=model, \n",
    "                                 param_distributions=param_grid,random_state=40,\n",
    "                                n_iter=200)\n",
    "\n",
    "rand_search.fit(X_scaled, Y)\n",
    "\n",
    "print(rand_search.best_estimator_.alpha)\n",
    "print(rand_search.best_score_)\n",
    "best_alpha=rand_search.best_estimator_.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=40)\n",
    "scaling= MinMaxScaler()\n",
    "X_train=scaling.fit_transform(X_train)\n",
    "X_test=scaling.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha=best_alpha)\n",
    "model_lasso.fit(X_train,y_train)\n",
    "y_pred_train=model_lasso.predict(X_train)\n",
    "y_pred_test=model_lasso.predict(X_test)\n",
    "r2train= r2_score(y_train,y_pred_train)\n",
    "r2test=r2_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8777022860095753, 0.8276239373798526)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2train,r2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error (test): 0.8227588507188169\n",
      "Root mean squared error (train): 0.7426521532305829\n"
     ]
    }
   ],
   "source": [
    "print('Root mean squared error (test):',np.sqrt(mean_squared_error(y_test,y_pred_test)))\n",
    "print('Root mean squared error (train):',np.sqrt(mean_squared_error(y_train,y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLR Model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))\n",
    "axes[0].scatter(y_test, y_pred_test,label=\"R^2test =\"+str(r2test))\n",
    "axes[0].plot(y_test,y_test,'r--')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[1].scatter(y_train, y_pred_train)\n",
    "axes[1].plot(y_test,y_test,'r--',label=\"R^2train =\"+str(r2train))\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train set 0.5515322206980212\n",
      "RMSE on train set 0.6769321264361485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE on train set\",mean_squared_error(y_train,y_pred_train))\n",
    "print(\"RMSE on train set\",mean_squared_error(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and significance (Parameter sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No of Signature descriptors selected using L1 feature selection method: 83\n"
     ]
    }
   ],
   "source": [
    "coeff = model_lasso.coef_\n",
    "a=[]\n",
    "for i in range(len(coeff)):\n",
    "    if(coeff[i]!=0):\n",
    "        a.append(i)\n",
    "XNN=X[:,a]\n",
    "print(\" No of Signature descriptors selected using L1 feature selection method:\",len(XNN[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={'weights':coeff[a].astype(float),'descriptor':uncor.columns[a]}\n",
    "new_df=pd.DataFrame(data=dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     weights                descriptor\n",
      "1  -7.288989                       [C]\n",
      "0  -5.702998                      [Cl]\n",
      "9  -3.243925     [C]([C]([C])[C]([C]))\n",
      "4  -3.157147                   [C]=[C]\n",
      "11 -2.818981                      [Br]\n",
      "8  -2.283436  [C]([C]=[C])[C]([C]=[C])\n",
      "7  -1.813920               [C]([C][C])\n",
      "62 -1.813215                   [P]=[S]\n",
      "17 -1.653292           [N]([C][O]=[O])\n",
      "41 -1.627357                       [I]\n",
      "21 -1.532294                       [S]\n",
      "6  -1.293141          [C]([C]=[C][Cl])\n",
      "\n",
      "\n",
      "     weights                descriptor\n",
      "31  0.536141                   [C]=[O]\n",
      "28  0.541980    [C]([C])[C]([C][C][O])\n",
      "38  0.546411          [C]([C])[C]([O])\n",
      "19  0.555033         [C](\\=\\[C][C])[O]\n",
      "64  0.706211                   [O]=[P]\n",
      "40  0.733317  [C]([C]([C])[C]([C][O]))\n",
      "56  0.774112    [C](\\=/[C])[C](\\=/[N])\n",
      "51  0.812611       [C]([C])\\=/[N]([C])\n",
      "52  1.197925    [C]([C])\\=/[C]([C][N])\n",
      "14  2.160589                  [O]([C])\n",
      "5   2.564086                       [O]\n"
     ]
    }
   ],
   "source": [
    "new_df.sort_values(by= [new_df.columns[0]],axis=0, ascending=True, inplace=True)\n",
    "print(new_df.head(12))\n",
    "print(\"\\n\")\n",
    "print(new_df.tail(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Neural Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(XNN, Y, test_size=0.20, random_state=40)\n",
    "scaling= MinMaxScaler()\n",
    "X_train=scaling.fit_transform(X_train)\n",
    "X_test=scaling.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 914 samples, validate on 229 samples\n",
      "Epoch 1/100\n",
      "914/914 [==============================] - 0s 322us/sample - loss: 11.9877 - val_loss: 8.6024\n",
      "Epoch 2/100\n",
      "914/914 [==============================] - 0s 60us/sample - loss: 7.4315 - val_loss: 4.4450\n",
      "Epoch 3/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 3.8785 - val_loss: 2.6739\n",
      "Epoch 4/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 2.6203 - val_loss: 1.9925\n",
      "Epoch 5/100\n",
      "914/914 [==============================] - 0s 56us/sample - loss: 1.9120 - val_loss: 1.5232\n",
      "Epoch 6/100\n",
      "914/914 [==============================] - 0s 55us/sample - loss: 1.4006 - val_loss: 1.2303\n",
      "Epoch 7/100\n",
      "914/914 [==============================] - 0s 56us/sample - loss: 1.0677 - val_loss: 1.0339\n",
      "Epoch 8/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.8397 - val_loss: 0.9149\n",
      "Epoch 9/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.6914 - val_loss: 0.8277\n",
      "Epoch 10/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.5898 - val_loss: 0.7797\n",
      "Epoch 11/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.5227 - val_loss: 0.7466\n",
      "Epoch 12/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.4675 - val_loss: 0.7213\n",
      "Epoch 13/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.4307 - val_loss: 0.6943\n",
      "Epoch 14/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.4047 - val_loss: 0.6864\n",
      "Epoch 15/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.3835 - val_loss: 0.6648\n",
      "Epoch 16/100\n",
      "914/914 [==============================] - 0s 64us/sample - loss: 0.3648 - val_loss: 0.6601\n",
      "Epoch 17/100\n",
      "914/914 [==============================] - 0s 60us/sample - loss: 0.3498 - val_loss: 0.6475\n",
      "Epoch 18/100\n",
      "914/914 [==============================] - 0s 60us/sample - loss: 0.3332 - val_loss: 0.6431\n",
      "Epoch 19/100\n",
      "914/914 [==============================] - 0s 66us/sample - loss: 0.3222 - val_loss: 0.6354\n",
      "Epoch 20/100\n",
      "914/914 [==============================] - 0s 60us/sample - loss: 0.3130 - val_loss: 0.6299\n",
      "Epoch 21/100\n",
      "914/914 [==============================] - 0s 64us/sample - loss: 0.3094 - val_loss: 0.6274\n",
      "Epoch 22/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.2985 - val_loss: 0.6244\n",
      "Epoch 23/100\n",
      "914/914 [==============================] - 0s 64us/sample - loss: 0.2892 - val_loss: 0.6213\n",
      "Epoch 24/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.2783 - val_loss: 0.6162\n",
      "Epoch 25/100\n",
      "914/914 [==============================] - 0s 64us/sample - loss: 0.2703 - val_loss: 0.6176\n",
      "Epoch 26/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.2671 - val_loss: 0.6129\n",
      "Epoch 27/100\n",
      "914/914 [==============================] - 0s 65us/sample - loss: 0.2648 - val_loss: 0.6038\n",
      "Epoch 28/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.2574 - val_loss: 0.6099\n",
      "Epoch 29/100\n",
      "914/914 [==============================] - 0s 64us/sample - loss: 0.2487 - val_loss: 0.5967\n",
      "Epoch 30/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.2401 - val_loss: 0.6068\n",
      "Epoch 31/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.2367 - val_loss: 0.5924\n",
      "Epoch 32/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.2307 - val_loss: 0.5946\n",
      "Epoch 33/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.2242 - val_loss: 0.5963\n",
      "Epoch 34/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.2202 - val_loss: 0.5962\n",
      "Epoch 35/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.2201 - val_loss: 0.5939\n",
      "Epoch 36/100\n",
      "914/914 [==============================] - 0s 64us/sample - loss: 0.2108 - val_loss: 0.5997\n",
      "Epoch 37/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.2095 - val_loss: 0.5902\n",
      "Epoch 38/100\n",
      "914/914 [==============================] - 0s 64us/sample - loss: 0.2045 - val_loss: 0.5864\n",
      "Epoch 39/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.2008 - val_loss: 0.5967\n",
      "Epoch 40/100\n",
      "914/914 [==============================] - 0s 65us/sample - loss: 0.2001 - val_loss: 0.5943\n",
      "Epoch 41/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1929 - val_loss: 0.5977\n",
      "Epoch 42/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1905 - val_loss: 0.5883\n",
      "Epoch 43/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1848 - val_loss: 0.5861\n",
      "Epoch 44/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1824 - val_loss: 0.5900\n",
      "Epoch 45/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.1810 - val_loss: 0.5806\n",
      "Epoch 46/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.1758 - val_loss: 0.5794\n",
      "Epoch 47/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1741 - val_loss: 0.5845\n",
      "Epoch 48/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1769 - val_loss: 0.5829\n",
      "Epoch 49/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1712 - val_loss: 0.5834\n",
      "Epoch 50/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.1680 - val_loss: 0.5895\n",
      "Epoch 51/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1670 - val_loss: 0.6015\n",
      "Epoch 52/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1641 - val_loss: 0.5850\n",
      "Epoch 53/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1616 - val_loss: 0.5879\n",
      "Epoch 54/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.1586 - val_loss: 0.5950\n",
      "Epoch 55/100\n",
      "914/914 [==============================] - 0s 63us/sample - loss: 0.1592 - val_loss: 0.5855\n",
      "Epoch 56/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.1523 - val_loss: 0.5926\n",
      "Epoch 57/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1487 - val_loss: 0.5795\n",
      "Epoch 58/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1470 - val_loss: 0.5826\n",
      "Epoch 59/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1464 - val_loss: 0.5814\n",
      "Epoch 60/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1449 - val_loss: 0.5833\n",
      "Epoch 61/100\n",
      "914/914 [==============================] - 0s 60us/sample - loss: 0.1420 - val_loss: 0.5932\n",
      "Epoch 62/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1430 - val_loss: 0.5909\n",
      "Epoch 63/100\n",
      "914/914 [==============================] - 0s 60us/sample - loss: 0.1394 - val_loss: 0.5887\n",
      "Epoch 64/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1372 - val_loss: 0.5942\n",
      "Epoch 65/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1357 - val_loss: 0.5920\n",
      "Epoch 66/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1327 - val_loss: 0.5853\n",
      "Epoch 67/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1314 - val_loss: 0.5891\n",
      "Epoch 68/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1427 - val_loss: 0.5998\n",
      "Epoch 69/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.1333 - val_loss: 0.5869\n",
      "Epoch 70/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1324 - val_loss: 0.6005\n",
      "Epoch 71/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1281 - val_loss: 0.5840\n",
      "Epoch 72/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1242 - val_loss: 0.5948\n",
      "Epoch 73/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1227 - val_loss: 0.5825\n",
      "Epoch 74/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1260 - val_loss: 0.5952\n",
      "Epoch 75/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.1207 - val_loss: 0.5882\n",
      "Epoch 76/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1187 - val_loss: 0.5880\n",
      "Epoch 77/100\n",
      "914/914 [==============================] - 0s 56us/sample - loss: 0.1205 - val_loss: 0.5917\n",
      "Epoch 78/100\n",
      "914/914 [==============================] - 0s 56us/sample - loss: 0.1172 - val_loss: 0.5831\n",
      "Epoch 79/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1180 - val_loss: 0.5900\n",
      "Epoch 80/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1142 - val_loss: 0.5901\n",
      "Epoch 81/100\n",
      "914/914 [==============================] - 0s 55us/sample - loss: 0.1161 - val_loss: 0.5820\n",
      "Epoch 82/100\n",
      "914/914 [==============================] - 0s 61us/sample - loss: 0.1142 - val_loss: 0.5969\n",
      "Epoch 83/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1131 - val_loss: 0.5857\n",
      "Epoch 84/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1124 - val_loss: 0.5960\n",
      "Epoch 85/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1121 - val_loss: 0.5945\n",
      "Epoch 86/100\n",
      "914/914 [==============================] - 0s 62us/sample - loss: 0.1088 - val_loss: 0.5980\n",
      "Epoch 87/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1085 - val_loss: 0.5979\n",
      "Epoch 88/100\n",
      "914/914 [==============================] - 0s 56us/sample - loss: 0.1081 - val_loss: 0.6040\n",
      "Epoch 89/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1081 - val_loss: 0.6069\n",
      "Epoch 90/100\n",
      "914/914 [==============================] - 0s 60us/sample - loss: 0.1059 - val_loss: 0.6031\n",
      "Epoch 91/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1045 - val_loss: 0.6011\n",
      "Epoch 92/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1042 - val_loss: 0.6013\n",
      "Epoch 93/100\n",
      "914/914 [==============================] - 0s 55us/sample - loss: 0.1048 - val_loss: 0.6007\n",
      "Epoch 94/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.1027 - val_loss: 0.6061\n",
      "Epoch 95/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1014 - val_loss: 0.6044\n",
      "Epoch 96/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1023 - val_loss: 0.6173\n",
      "Epoch 97/100\n",
      "914/914 [==============================] - 0s 57us/sample - loss: 0.1057 - val_loss: 0.6040\n",
      "Epoch 98/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.1081 - val_loss: 0.6075\n",
      "Epoch 99/100\n",
      "914/914 [==============================] - 0s 59us/sample - loss: 0.0983 - val_loss: 0.6092\n",
      "Epoch 100/100\n",
      "914/914 [==============================] - 0s 58us/sample - loss: 0.0981 - val_loss: 0.6057\n"
     ]
    }
   ],
   "source": [
    "#baseline model\n",
    "baseline_model = Sequential([\n",
    "  Dense(len(X_train[0]), activation='relu'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1,activation='linear')])\n",
    "# Compile the model.\n",
    "baseline_model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "# Train the model.\n",
    "history = baseline_model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  epochs=100,\n",
    "  validation_data = (X_test,y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared (test set): 0.8457515770354711\n",
      "Mean squared error (test set): 0.6057436941630017\n"
     ]
    }
   ],
   "source": [
    "#performance of baseline model\n",
    "Y_pred=baseline_model.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "print('R squared (test set):',r2_score(y_test,Y_pred))\n",
    "print('Mean squared error (test set):',mean_squared_error(y_test,Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of the neural network model to improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining callback to save the best model during training\n",
    "def model_callbacks(weight_file='Weights_v3{epoch:02d}.h5'):  \n",
    "    checkpoint = ModelCheckpoint(weight_file, monitor = 'val_loss', verbose=1, save_best_only= True,save_weights_only= True, mode = 'min')\n",
    "    #checkpoint = ModelCheckpoint(weight_file, monitor = 'val_loss', verbose=1)\n",
    "    return [checkpoint]\n",
    "#defining function to grid search best hyperparameter for learning rate and batch_size\n",
    "def my_nn_model(alpha=0.001):\n",
    "    model = Sequential([\n",
    "    Dense(len(X_train[0]), activation='relu',activity_regularizer = regularizers.l2(1e-3)),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu',activity_regularizer = regularizers.l2(1e-4)),\n",
    "    Dropout(0.01),\n",
    "    Dense(1,activation='linear')])\n",
    "    model.compile(optimizer=Adam(learning_rate=alpha),loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 12.93852, saving model to Weights_gs001.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 12.93852 to 12.47231, saving model to Weights_gs002.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 12.47231 to 11.90266, saving model to Weights_gs003.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 11.90266 to 11.12296, saving model to Weights_gs004.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 11.12296 to 10.22895, saving model to Weights_gs005.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 10.22895 to 9.34664, saving model to Weights_gs006.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 9.34664 to 8.47093, saving model to Weights_gs007.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 8.47093 to 7.60100, saving model to Weights_gs008.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 7.60100 to 6.73494, saving model to Weights_gs009.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 6.73494 to 5.91200, saving model to Weights_gs010.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 5.91200 to 5.16066, saving model to Weights_gs011.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 5.16066 to 4.51315, saving model to Weights_gs012.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 4.51315 to 3.96246, saving model to Weights_gs013.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.96246 to 3.55141, saving model to Weights_gs014.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.55141 to 3.22674, saving model to Weights_gs015.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.22674 to 2.98548, saving model to Weights_gs016.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.98548 to 2.78446, saving model to Weights_gs017.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.78446 to 2.63017, saving model to Weights_gs018.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.63017 to 2.50846, saving model to Weights_gs019.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.50846 to 2.40531, saving model to Weights_gs020.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.40531 to 2.31267, saving model to Weights_gs021.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.31267 to 2.22143, saving model to Weights_gs022.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.22143 to 2.14104, saving model to Weights_gs023.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.14104 to 2.06295, saving model to Weights_gs024.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.06295 to 1.98200, saving model to Weights_gs025.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.98200 to 1.90766, saving model to Weights_gs026.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.90766 to 1.83836, saving model to Weights_gs027.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.83836 to 1.77275, saving model to Weights_gs028.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.77275 to 1.71243, saving model to Weights_gs029.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.71243 to 1.65390, saving model to Weights_gs030.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.65390 to 1.59488, saving model to Weights_gs031.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.59488 to 1.53565, saving model to Weights_gs032.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.53565 to 1.48106, saving model to Weights_gs033.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.48106 to 1.43163, saving model to Weights_gs034.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.43163 to 1.38093, saving model to Weights_gs035.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.38093 to 1.33308, saving model to Weights_gs036.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.33308 to 1.28643, saving model to Weights_gs037.h5\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.28643 to 1.24073, saving model to Weights_gs038.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.24073 to 1.19828, saving model to Weights_gs039.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.19828 to 1.15711, saving model to Weights_gs040.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.15711 to 1.11450, saving model to Weights_gs041.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.11450 to 1.07207, saving model to Weights_gs042.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.07207 to 1.03575, saving model to Weights_gs043.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.03575 to 1.00085, saving model to Weights_gs044.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.00085 to 0.96817, saving model to Weights_gs045.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.96817 to 0.94096, saving model to Weights_gs046.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.94096 to 0.91477, saving model to Weights_gs047.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.91477 to 0.89231, saving model to Weights_gs048.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.89231 to 0.86811, saving model to Weights_gs049.h5\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.86811 to 0.85252, saving model to Weights_gs050.h5\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.85252 to 0.83715, saving model to Weights_gs051.h5\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.83715 to 0.82570, saving model to Weights_gs052.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.82570 to 0.81023, saving model to Weights_gs053.h5\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.81023 to 0.80001, saving model to Weights_gs054.h5\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.80001 to 0.79142, saving model to Weights_gs055.h5\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.79142 to 0.78368, saving model to Weights_gs056.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.78368 to 0.77580, saving model to Weights_gs057.h5\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.77580 to 0.76660, saving model to Weights_gs058.h5\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.76660 to 0.75885, saving model to Weights_gs059.h5\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.75885 to 0.75528, saving model to Weights_gs060.h5\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.75528 to 0.74627, saving model to Weights_gs061.h5\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.74627 to 0.74522, saving model to Weights_gs062.h5\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.74522 to 0.73961, saving model to Weights_gs063.h5\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.73961 to 0.73059, saving model to Weights_gs064.h5\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.73059 to 0.72803, saving model to Weights_gs065.h5\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.72803 to 0.72331, saving model to Weights_gs066.h5\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.72331 to 0.71942, saving model to Weights_gs067.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.71942 to 0.71300, saving model to Weights_gs068.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.71300 to 0.70924, saving model to Weights_gs069.h5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.70924 to 0.70640, saving model to Weights_gs070.h5\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.70640 to 0.70183, saving model to Weights_gs071.h5\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.70183 to 0.70069, saving model to Weights_gs072.h5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.70069 to 0.69676, saving model to Weights_gs073.h5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.69676 to 0.69499, saving model to Weights_gs074.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.69499\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.69499 to 0.69049, saving model to Weights_gs076.h5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.69049 to 0.68830, saving model to Weights_gs077.h5\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.68830 to 0.68709, saving model to Weights_gs078.h5\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.68709 to 0.68509, saving model to Weights_gs079.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.68509 to 0.68211, saving model to Weights_gs080.h5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.68211 to 0.67945, saving model to Weights_gs081.h5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.67945\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.67945 to 0.67668, saving model to Weights_gs083.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.67668 to 0.67444, saving model to Weights_gs084.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.67444 to 0.67327, saving model to Weights_gs085.h5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.67327 to 0.67232, saving model to Weights_gs086.h5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.67232 to 0.66941, saving model to Weights_gs087.h5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.66941 to 0.66616, saving model to Weights_gs088.h5\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.66616\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.66616 to 0.66319, saving model to Weights_gs090.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.66319 to 0.66254, saving model to Weights_gs091.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.66254\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.66254 to 0.65914, saving model to Weights_gs093.h5\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.65914 to 0.65587, saving model to Weights_gs094.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_loss did not improve from 0.65587\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.65587\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.65587 to 0.65584, saving model to Weights_gs097.h5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.65584 to 0.65453, saving model to Weights_gs098.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.65453 to 0.65444, saving model to Weights_gs099.h5\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.65444 to 0.65198, saving model to Weights_gs0100.h5\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.65198 to 0.65114, saving model to Weights_gs0101.h5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.65114\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.65114 to 0.64688, saving model to Weights_gs0103.h5\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.64688 to 0.64666, saving model to Weights_gs0104.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.64666 to 0.64516, saving model to Weights_gs0105.h5\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.64516 to 0.64379, saving model to Weights_gs0106.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.64379 to 0.64321, saving model to Weights_gs0107.h5\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.64321 to 0.64045, saving model to Weights_gs0108.h5\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.64045\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.64045 to 0.64017, saving model to Weights_gs0110.h5\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.64017 to 0.63804, saving model to Weights_gs0111.h5\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.63804 to 0.63713, saving model to Weights_gs0112.h5\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.63713 to 0.63636, saving model to Weights_gs0113.h5\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.63636 to 0.63534, saving model to Weights_gs0114.h5\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.63534\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.63534\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.63534\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.63534\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.63534 to 0.63307, saving model to Weights_gs0119.h5\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.63307\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.63307 to 0.63148, saving model to Weights_gs0121.h5\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.63148 to 0.63080, saving model to Weights_gs0122.h5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.63080\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.63080\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.63080 to 0.63079, saving model to Weights_gs0125.h5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.63079 to 0.62964, saving model to Weights_gs0126.h5\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.62964 to 0.62895, saving model to Weights_gs0127.h5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.62895 to 0.62824, saving model to Weights_gs0128.h5\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.62824 to 0.62658, saving model to Weights_gs0129.h5\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.62658 to 0.62588, saving model to Weights_gs0130.h5\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.62588 to 0.62434, saving model to Weights_gs0131.h5\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.62434\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.62434\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.62434\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.62434 to 0.62227, saving model to Weights_gs0135.h5\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.62227 to 0.62149, saving model to Weights_gs0136.h5\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.62149\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.62149\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.62149\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.62149\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.62149\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.62149\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.62149\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.62149 to 0.61882, saving model to Weights_gs0144.h5\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.61882 to 0.61668, saving model to Weights_gs0145.h5\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.61668 to 0.61232, saving model to Weights_gs0146.h5\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.61232 to 0.61185, saving model to Weights_gs0147.h5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.61185\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.61185 to 0.61064, saving model to Weights_gs0149.h5\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.61064\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.61064 to 0.61031, saving model to Weights_gs0158.h5\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.61031\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.61031 to 0.60953, saving model to Weights_gs0160.h5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.60953\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.60953\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.60953\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.60953\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.60953\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.60953\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.60953\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.60953 to 0.60941, saving model to Weights_gs0168.h5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.60941 to 0.60781, saving model to Weights_gs0169.h5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.60781\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.60781 to 0.60780, saving model to Weights_gs0171.h5\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.60780\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.60780 to 0.60778, saving model to Weights_gs0173.h5\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.60778 to 0.60432, saving model to Weights_gs0174.h5\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.60432 to 0.60315, saving model to Weights_gs0175.h5\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.60315\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.60315 to 0.60234, saving model to Weights_gs0191.h5\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.60234 to 0.60224, saving model to Weights_gs0192.h5\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.60224 to 0.59889, saving model to Weights_gs0193.h5\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.59889\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.59889\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.59889\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.59889 to 0.59798, saving model to Weights_gs0197.h5\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.59798\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.59798\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.59798\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.59798\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.59798\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.59798 to 0.59686, saving model to Weights_gs0203.h5\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.59686 to 0.59580, saving model to Weights_gs0204.h5\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.59580 to 0.59520, saving model to Weights_gs0205.h5\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.59520\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.59520 to 0.59382, saving model to Weights_gs0214.h5\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.59382 to 0.59267, saving model to Weights_gs0215.h5\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.59267 to 0.59235, saving model to Weights_gs0216.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00217: val_loss improved from 0.59235 to 0.59196, saving model to Weights_gs0217.h5\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.59196\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.59196\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.59196\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.59196 to 0.59166, saving model to Weights_gs0221.h5\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.59166\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.59166 to 0.59141, saving model to Weights_gs0236.h5\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.59141 to 0.58780, saving model to Weights_gs0237.h5\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.58780\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.58780 to 0.58673, saving model to Weights_gs0250.h5\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.58673 to 0.58443, saving model to Weights_gs0251.h5\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.58443\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.58443 to 0.58434, saving model to Weights_gs0282.h5\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.58434 to 0.58338, saving model to Weights_gs0283.h5\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.58338 to 0.58335, saving model to Weights_gs0284.h5\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.58335\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.58335 to 0.58181, saving model to Weights_gs0297.h5\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.58181\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.58181\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.58181\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.58181\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.58181\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.58181 to 0.58008, saving model to Weights_gs0303.h5\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.58008 to 0.57947, saving model to Weights_gs0304.h5\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.57947\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.57947 to 0.57917, saving model to Weights_gs0313.h5\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.57917 to 0.57877, saving model to Weights_gs0314.h5\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.57877 to 0.57790, saving model to Weights_gs0315.h5\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.57790 to 0.57733, saving model to Weights_gs0316.h5\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.57733\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.57733 to 0.57667, saving model to Weights_gs0334.h5\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.57667\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.57667\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.57667 to 0.57500, saving model to Weights_gs0337.h5\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.57500\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.57500 to 0.57393, saving model to Weights_gs0339.h5\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.57393\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.57393\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.57393\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.57393\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.57393\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.57393 to 0.57362, saving model to Weights_gs0345.h5\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.57362\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.57362\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.57362 to 0.57356, saving model to Weights_gs0348.h5\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.57356\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.57356 to 0.57118, saving model to Weights_gs0350.h5\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.57118\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.57118 to 0.57043, saving model to Weights_gs0352.h5\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.57043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00359: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.57043\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.57043 to 0.56692, saving model to Weights_gs0426.h5\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.56692\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.56692 to 0.56617, saving model to Weights_gs0438.h5\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.56617\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.56617 to 0.56545, saving model to Weights_gs0473.h5\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.56545\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.56545\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.56545 to 0.56353, saving model to Weights_gs0476.h5\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.56353\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.56353\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.56353\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.56353\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.56353\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.56353\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.56353\n",
      "\n",
      "Epoch 00484: val_loss improved from 0.56353 to 0.56343, saving model to Weights_gs0484.h5\n",
      "\n",
      "Epoch 00485: val_loss improved from 0.56343 to 0.56325, saving model to Weights_gs0485.h5\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.71361, saving model to Weights_gs101.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.71361 to 2.30592, saving model to Weights_gs102.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.30592 to 1.47713, saving model to Weights_gs103.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47713 to 1.02969, saving model to Weights_gs104.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.02969 to 0.85439, saving model to Weights_gs105.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.85439 to 0.76929, saving model to Weights_gs106.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.76929 to 0.73765, saving model to Weights_gs107.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss improved from 0.73765 to 0.69784, saving model to Weights_gs108.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69784 to 0.68078, saving model to Weights_gs109.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.68078 to 0.66699, saving model to Weights_gs110.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.66699 to 0.66002, saving model to Weights_gs111.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.66002 to 0.64363, saving model to Weights_gs112.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.64363 to 0.63297, saving model to Weights_gs113.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.63297 to 0.63039, saving model to Weights_gs114.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.63039 to 0.62300, saving model to Weights_gs115.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.62300 to 0.61650, saving model to Weights_gs116.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.61650\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.61650 to 0.61336, saving model to Weights_gs118.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.61336\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.61336 to 0.61105, saving model to Weights_gs120.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.61105\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.61105 to 0.59499, saving model to Weights_gs122.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.59499\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.59499\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.59499\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.59499\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.59499 to 0.59391, saving model to Weights_gs127.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.59391\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.59391\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.59391\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.59391 to 0.57373, saving model to Weights_gs131.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.57373\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.57373\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.57373\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.57373 to 0.57032, saving model to Weights_gs135.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.57032\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.57032 to 0.55365, saving model to Weights_gs137.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.55365\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.55365 to 0.54377, saving model to Weights_gs154.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.54377\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.54377\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.54377\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.54377 to 0.53950, saving model to Weights_gs158.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.53950\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.53950\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.53950 to 0.53703, saving model to Weights_gs161.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.53703\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.53703\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.53703 to 0.53556, saving model to Weights_gs164.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.53556\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.53556 to 0.53127, saving model to Weights_gs166.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.53127\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.53127 to 0.52852, saving model to Weights_gs180.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.52852\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.52852\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.52852\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.52852 to 0.52315, saving model to Weights_gs184.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.52315\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.52315 to 0.51735, saving model to Weights_gs1118.h5\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.51735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00149: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.51735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00308: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.51735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00468: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.51735\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25670, saving model to Weights_gs201.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.25670\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.25670 to 0.96434, saving model to Weights_gs203.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.96434\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.96434\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.96434 to 0.79731, saving model to Weights_gs206.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.79731 to 0.74289, saving model to Weights_gs207.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.74289\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.74289 to 0.72313, saving model to Weights_gs209.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.72313\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.72313 to 0.66042, saving model to Weights_gs211.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.66042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00123: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.66042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00281: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.66042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00440: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.66042\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95956, saving model to Weights_gs301.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95956 to 0.76916, saving model to Weights_gs302.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76916 to 0.72892, saving model to Weights_gs303.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.72892\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.72892 to 0.67585, saving model to Weights_gs305.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.67585 to 0.67010, saving model to Weights_gs306.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.67010\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.67010 to 0.66054, saving model to Weights_gs308.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66054\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66054\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66054\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.66054 to 0.62303, saving model to Weights_gs312.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.62303 to 0.62222, saving model to Weights_gs313.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.62222 to 0.60718, saving model to Weights_gs314.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60718\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.60718\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.60718\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.60718 to 0.60597, saving model to Weights_gs318.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.60597 to 0.59530, saving model to Weights_gs319.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.59530\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.59530\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.59530\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.59530\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.59530\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.59530\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.59530\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.59530 to 0.58705, saving model to Weights_gs327.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.58705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00089: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.58705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00248: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.58705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00406: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.58705\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 12.26294, saving model to Weights_gs401.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 12.26294 to 11.97945, saving model to Weights_gs402.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 11.97945 to 11.67897, saving model to Weights_gs403.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 11.67897 to 11.35132, saving model to Weights_gs404.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 11.35132 to 10.99181, saving model to Weights_gs405.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 10.99181 to 10.59100, saving model to Weights_gs406.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 10.59100 to 10.15353, saving model to Weights_gs407.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 10.15353 to 9.69370, saving model to Weights_gs408.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 9.69370 to 9.21229, saving model to Weights_gs409.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 9.21229 to 8.72671, saving model to Weights_gs410.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 8.72671 to 8.22193, saving model to Weights_gs411.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 8.22193 to 7.72148, saving model to Weights_gs412.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 7.72148 to 7.21796, saving model to Weights_gs413.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 7.21796 to 6.71118, saving model to Weights_gs414.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.71118 to 6.22276, saving model to Weights_gs415.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 6.22276 to 5.75461, saving model to Weights_gs416.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 5.75461 to 5.32943, saving model to Weights_gs417.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 5.32943 to 4.91285, saving model to Weights_gs418.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 4.91285 to 4.54240, saving model to Weights_gs419.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 4.54240 to 4.22547, saving model to Weights_gs420.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 4.22547 to 3.93188, saving model to Weights_gs421.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.93188 to 3.68751, saving model to Weights_gs422.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 3.68751 to 3.48043, saving model to Weights_gs423.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 3.48043 to 3.30848, saving model to Weights_gs424.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 3.30848 to 3.16121, saving model to Weights_gs425.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 3.16121 to 3.03993, saving model to Weights_gs426.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.03993 to 2.92566, saving model to Weights_gs427.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.92566 to 2.83176, saving model to Weights_gs428.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.83176 to 2.74330, saving model to Weights_gs429.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.74330 to 2.66559, saving model to Weights_gs430.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 2.66559 to 2.59264, saving model to Weights_gs431.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.59264 to 2.52088, saving model to Weights_gs432.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 2.52088 to 2.45648, saving model to Weights_gs433.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 2.45648 to 2.39527, saving model to Weights_gs434.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 2.39527 to 2.33110, saving model to Weights_gs435.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 2.33110 to 2.27261, saving model to Weights_gs436.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 2.27261 to 2.21564, saving model to Weights_gs437.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_loss improved from 2.21564 to 2.16086, saving model to Weights_gs438.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 2.16086 to 2.10967, saving model to Weights_gs439.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 2.10967 to 2.05648, saving model to Weights_gs440.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 2.05648 to 2.00654, saving model to Weights_gs441.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 2.00654 to 1.95584, saving model to Weights_gs442.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.95584 to 1.90824, saving model to Weights_gs443.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.90824 to 1.86202, saving model to Weights_gs444.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.86202 to 1.81685, saving model to Weights_gs445.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.81685 to 1.77331, saving model to Weights_gs446.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.77331 to 1.73176, saving model to Weights_gs447.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.73176 to 1.68986, saving model to Weights_gs448.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.68986 to 1.64923, saving model to Weights_gs449.h5\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.64923 to 1.60875, saving model to Weights_gs450.h5\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.60875 to 1.57075, saving model to Weights_gs451.h5\n",
      "\n",
      "Epoch 00052: val_loss improved from 1.57075 to 1.53398, saving model to Weights_gs452.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 1.53398 to 1.49881, saving model to Weights_gs453.h5\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.49881 to 1.46499, saving model to Weights_gs454.h5\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.46499 to 1.43261, saving model to Weights_gs455.h5\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.43261 to 1.39859, saving model to Weights_gs456.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.39859 to 1.36665, saving model to Weights_gs457.h5\n",
      "\n",
      "Epoch 00058: val_loss improved from 1.36665 to 1.33766, saving model to Weights_gs458.h5\n",
      "\n",
      "Epoch 00059: val_loss improved from 1.33766 to 1.30884, saving model to Weights_gs459.h5\n",
      "\n",
      "Epoch 00060: val_loss improved from 1.30884 to 1.28049, saving model to Weights_gs460.h5\n",
      "\n",
      "Epoch 00061: val_loss improved from 1.28049 to 1.25522, saving model to Weights_gs461.h5\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.25522 to 1.22882, saving model to Weights_gs462.h5\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.22882 to 1.20381, saving model to Weights_gs463.h5\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.20381 to 1.18041, saving model to Weights_gs464.h5\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.18041 to 1.15702, saving model to Weights_gs465.h5\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.15702 to 1.13517, saving model to Weights_gs466.h5\n",
      "\n",
      "Epoch 00067: val_loss improved from 1.13517 to 1.11474, saving model to Weights_gs467.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 1.11474 to 1.09549, saving model to Weights_gs468.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.09549 to 1.07612, saving model to Weights_gs469.h5\n",
      "\n",
      "Epoch 00070: val_loss improved from 1.07612 to 1.05827, saving model to Weights_gs470.h5\n",
      "\n",
      "Epoch 00071: val_loss improved from 1.05827 to 1.04158, saving model to Weights_gs471.h5\n",
      "\n",
      "Epoch 00072: val_loss improved from 1.04158 to 1.02475, saving model to Weights_gs472.h5\n",
      "\n",
      "Epoch 00073: val_loss improved from 1.02475 to 1.00746, saving model to Weights_gs473.h5\n",
      "\n",
      "Epoch 00074: val_loss improved from 1.00746 to 0.99162, saving model to Weights_gs474.h5\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.99162 to 0.97742, saving model to Weights_gs475.h5\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.97742 to 0.96403, saving model to Weights_gs476.h5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.96403 to 0.95192, saving model to Weights_gs477.h5\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.95192 to 0.93893, saving model to Weights_gs478.h5\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.93893 to 0.92621, saving model to Weights_gs479.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.92621 to 0.91619, saving model to Weights_gs480.h5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.91619 to 0.90531, saving model to Weights_gs481.h5\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.90531 to 0.89511, saving model to Weights_gs482.h5\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.89511 to 0.88690, saving model to Weights_gs483.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.88690 to 0.87927, saving model to Weights_gs484.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.87927 to 0.86974, saving model to Weights_gs485.h5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.86974 to 0.86041, saving model to Weights_gs486.h5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.86041 to 0.85283, saving model to Weights_gs487.h5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.85283 to 0.84533, saving model to Weights_gs488.h5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.84533 to 0.83915, saving model to Weights_gs489.h5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.83915 to 0.83368, saving model to Weights_gs490.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.83368 to 0.82721, saving model to Weights_gs491.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.82721 to 0.82133, saving model to Weights_gs492.h5\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.82133 to 0.81479, saving model to Weights_gs493.h5\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.81479 to 0.81111, saving model to Weights_gs494.h5\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.81111 to 0.80610, saving model to Weights_gs495.h5\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.80610 to 0.80093, saving model to Weights_gs496.h5\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.80093 to 0.79582, saving model to Weights_gs497.h5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.79582 to 0.78976, saving model to Weights_gs498.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.78976 to 0.78344, saving model to Weights_gs499.h5\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.78344 to 0.78084, saving model to Weights_gs4100.h5\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.78084 to 0.77577, saving model to Weights_gs4101.h5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.77577 to 0.77055, saving model to Weights_gs4102.h5\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.77055 to 0.76787, saving model to Weights_gs4103.h5\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.76787 to 0.76492, saving model to Weights_gs4104.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.76492 to 0.76213, saving model to Weights_gs4105.h5\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.76213 to 0.75700, saving model to Weights_gs4106.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.75700 to 0.75315, saving model to Weights_gs4107.h5\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.75315 to 0.74911, saving model to Weights_gs4108.h5\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.74911 to 0.74523, saving model to Weights_gs4109.h5\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.74523 to 0.74241, saving model to Weights_gs4110.h5\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.74241 to 0.73947, saving model to Weights_gs4111.h5\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.73947 to 0.73775, saving model to Weights_gs4112.h5\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.73775 to 0.73581, saving model to Weights_gs4113.h5\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.73581 to 0.73243, saving model to Weights_gs4114.h5\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.73243 to 0.72898, saving model to Weights_gs4115.h5\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.72898 to 0.72707, saving model to Weights_gs4116.h5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.72707 to 0.72647, saving model to Weights_gs4117.h5\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.72647 to 0.72307, saving model to Weights_gs4118.h5\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.72307 to 0.71912, saving model to Weights_gs4119.h5\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.71912 to 0.71586, saving model to Weights_gs4120.h5\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.71586 to 0.71501, saving model to Weights_gs4121.h5\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.71501 to 0.71285, saving model to Weights_gs4122.h5\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.71285 to 0.71229, saving model to Weights_gs4123.h5\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.71229 to 0.70855, saving model to Weights_gs4124.h5\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.70855 to 0.70749, saving model to Weights_gs4125.h5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.70749 to 0.70362, saving model to Weights_gs4126.h5\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.70362 to 0.70113, saving model to Weights_gs4127.h5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.70113 to 0.69911, saving model to Weights_gs4128.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00129: val_loss improved from 0.69911 to 0.69543, saving model to Weights_gs4129.h5\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.69543 to 0.69383, saving model to Weights_gs4130.h5\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.69383 to 0.69121, saving model to Weights_gs4131.h5\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.69121 to 0.68940, saving model to Weights_gs4132.h5\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.68940 to 0.68809, saving model to Weights_gs4133.h5\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.68809 to 0.68625, saving model to Weights_gs4134.h5\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.68625 to 0.68413, saving model to Weights_gs4135.h5\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.68413 to 0.68169, saving model to Weights_gs4136.h5\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.68169 to 0.67970, saving model to Weights_gs4137.h5\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.67970 to 0.67805, saving model to Weights_gs4138.h5\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.67805 to 0.67688, saving model to Weights_gs4139.h5\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.67688 to 0.67402, saving model to Weights_gs4140.h5\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.67402 to 0.67313, saving model to Weights_gs4141.h5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.67313\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.67313 to 0.67135, saving model to Weights_gs4143.h5\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.67135\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.67135 to 0.67070, saving model to Weights_gs4145.h5\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.67070 to 0.66946, saving model to Weights_gs4146.h5\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.66946\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.66946 to 0.66735, saving model to Weights_gs4148.h5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.66735 to 0.66506, saving model to Weights_gs4149.h5\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.66506 to 0.66318, saving model to Weights_gs4150.h5\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.66318 to 0.66304, saving model to Weights_gs4151.h5\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.66304 to 0.66228, saving model to Weights_gs4152.h5\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.66228 to 0.66060, saving model to Weights_gs4153.h5\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.66060 to 0.66052, saving model to Weights_gs4154.h5\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.66052 to 0.65944, saving model to Weights_gs4155.h5\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.65944 to 0.65886, saving model to Weights_gs4156.h5\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.65886 to 0.65693, saving model to Weights_gs4157.h5\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.65693 to 0.65546, saving model to Weights_gs4158.h5\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.65546 to 0.65514, saving model to Weights_gs4159.h5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.65514 to 0.65353, saving model to Weights_gs4160.h5\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.65353 to 0.65264, saving model to Weights_gs4161.h5\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.65264 to 0.65088, saving model to Weights_gs4162.h5\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.65088 to 0.64974, saving model to Weights_gs4163.h5\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.64974 to 0.64919, saving model to Weights_gs4164.h5\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.64919 to 0.64855, saving model to Weights_gs4165.h5\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.64855 to 0.64600, saving model to Weights_gs4166.h5\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.64600\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.64600\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.64600 to 0.64543, saving model to Weights_gs4169.h5\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.64543 to 0.64278, saving model to Weights_gs4170.h5\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.64278 to 0.64114, saving model to Weights_gs4171.h5\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.64114 to 0.64108, saving model to Weights_gs4172.h5\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.64108 to 0.63998, saving model to Weights_gs4173.h5\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.63998 to 0.63852, saving model to Weights_gs4174.h5\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.63852 to 0.63705, saving model to Weights_gs4175.h5\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.63705 to 0.63704, saving model to Weights_gs4176.h5\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.63704 to 0.63581, saving model to Weights_gs4177.h5\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.63581\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.63581 to 0.63396, saving model to Weights_gs4179.h5\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.63396\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.63396 to 0.63131, saving model to Weights_gs4181.h5\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.63131\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.63131 to 0.62935, saving model to Weights_gs4183.h5\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.62935 to 0.62919, saving model to Weights_gs4184.h5\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.62919\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.62919 to 0.62869, saving model to Weights_gs4186.h5\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.62869\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.62869\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.62869 to 0.62864, saving model to Weights_gs4189.h5\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.62864 to 0.62818, saving model to Weights_gs4190.h5\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.62818\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.62818 to 0.62759, saving model to Weights_gs4192.h5\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.62759\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.62759 to 0.62578, saving model to Weights_gs4194.h5\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.62578 to 0.62506, saving model to Weights_gs4195.h5\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.62506 to 0.62405, saving model to Weights_gs4196.h5\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.62405 to 0.62251, saving model to Weights_gs4197.h5\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.62251\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.62251 to 0.62135, saving model to Weights_gs4199.h5\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.62135\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.62135 to 0.62062, saving model to Weights_gs4211.h5\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.62062 to 0.62004, saving model to Weights_gs4212.h5\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.62004\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.62004 to 0.61938, saving model to Weights_gs4214.h5\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.61938 to 0.61884, saving model to Weights_gs4215.h5\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.61884 to 0.61865, saving model to Weights_gs4216.h5\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.61865\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.61865 to 0.61777, saving model to Weights_gs4227.h5\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.61777\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.61777 to 0.61712, saving model to Weights_gs4229.h5\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.61712 to 0.61685, saving model to Weights_gs4230.h5\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.61685 to 0.61639, saving model to Weights_gs4231.h5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.61639\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.61639\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.61639 to 0.61570, saving model to Weights_gs4234.h5\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.61570 to 0.61451, saving model to Weights_gs4235.h5\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.61451\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.61451 to 0.61382, saving model to Weights_gs4237.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00238: val_loss improved from 0.61382 to 0.61206, saving model to Weights_gs4238.h5\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.61206\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.61206\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.61206\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.61206\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.61206\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.61206\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.61206 to 0.61159, saving model to Weights_gs4245.h5\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.61159\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.61159\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.61159 to 0.60963, saving model to Weights_gs4248.h5\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.60963 to 0.60959, saving model to Weights_gs4249.h5\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.60959 to 0.60959, saving model to Weights_gs4250.h5\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.60959\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.60959\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.60959 to 0.60894, saving model to Weights_gs4253.h5\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.60894\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.60894\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.60894\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.60894 to 0.60894, saving model to Weights_gs4257.h5\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.60894\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.60894 to 0.60856, saving model to Weights_gs4259.h5\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.60856\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.60856\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.60856\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.60856 to 0.60853, saving model to Weights_gs4263.h5\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.60853 to 0.60824, saving model to Weights_gs4264.h5\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.60824\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.60824\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.60824\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.60824\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.60824\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.60824 to 0.60808, saving model to Weights_gs4270.h5\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.60808\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.60808 to 0.60700, saving model to Weights_gs4272.h5\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.60700\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.60700 to 0.60637, saving model to Weights_gs4287.h5\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.60637\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.60637\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.60637 to 0.60553, saving model to Weights_gs4290.h5\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.60553\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.60553\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.60553\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.60553\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.60553 to 0.60480, saving model to Weights_gs4295.h5\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.60480 to 0.60350, saving model to Weights_gs4296.h5\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.60350\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.60350 to 0.60268, saving model to Weights_gs4298.h5\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.60268\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.60268 to 0.60184, saving model to Weights_gs4300.h5\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.60184\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.60184 to 0.60174, saving model to Weights_gs4320.h5\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.60174\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.60174\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.60174\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.60174\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.60174\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.60174 to 0.60074, saving model to Weights_gs4326.h5\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.60074\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.60074\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.60074 to 0.60005, saving model to Weights_gs4329.h5\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.60005\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.60005\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.60005\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.60005\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.60005\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.60005\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.60005 to 0.59952, saving model to Weights_gs4336.h5\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.59952 to 0.59851, saving model to Weights_gs4337.h5\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.59851\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.59851 to 0.59848, saving model to Weights_gs4363.h5\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.59848\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.59848 to 0.59648, saving model to Weights_gs4374.h5\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.59648\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.59648\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.59648\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.59648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00379: val_loss did not improve from 0.59648\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.59648 to 0.59610, saving model to Weights_gs4380.h5\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.59610 to 0.59541, saving model to Weights_gs4381.h5\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.59541\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.59541 to 0.59509, saving model to Weights_gs4383.h5\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.59509\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.59509\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.59509\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.59509\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.59509\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.59509\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.59509\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.59509 to 0.59398, saving model to Weights_gs4391.h5\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.59398 to 0.59340, saving model to Weights_gs4392.h5\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.59340\n",
      "\n",
      "Epoch 00467: val_loss improved from 0.59340 to 0.59238, saving model to Weights_gs4467.h5\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.59238\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.59238\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.59238\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.59238\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.59238\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.59238 to 0.59183, saving model to Weights_gs4473.h5\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.59183 to 0.59106, saving model to Weights_gs4474.h5\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.59106\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.59106 to 0.59071, saving model to Weights_gs4497.h5\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.59071\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.59071\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.59071\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.51844, saving model to Weights_gs501.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.51844 to 3.35216, saving model to Weights_gs502.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.35216 to 2.39710, saving model to Weights_gs503.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.39710 to 1.78019, saving model to Weights_gs504.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.78019 to 1.37138, saving model to Weights_gs505.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.37138 to 1.11734, saving model to Weights_gs506.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.11734 to 0.95451, saving model to Weights_gs507.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.95451 to 0.86163, saving model to Weights_gs508.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.86163 to 0.80086, saving model to Weights_gs509.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.80086 to 0.77859, saving model to Weights_gs510.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.77859 to 0.73962, saving model to Weights_gs511.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.73962 to 0.71083, saving model to Weights_gs512.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.71083 to 0.70450, saving model to Weights_gs513.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.70450 to 0.67745, saving model to Weights_gs514.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.67745 to 0.66810, saving model to Weights_gs515.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.66810 to 0.65167, saving model to Weights_gs516.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.65167 to 0.64214, saving model to Weights_gs517.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.64214 to 0.63823, saving model to Weights_gs518.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.63823 to 0.63061, saving model to Weights_gs519.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_loss did not improve from 0.63061\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.63061\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.63061\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.63061\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.63061 to 0.63048, saving model to Weights_gs524.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.63048 to 0.62911, saving model to Weights_gs525.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.62911\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.62911 to 0.62870, saving model to Weights_gs527.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.62870\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.62870\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.62870 to 0.61552, saving model to Weights_gs530.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.61552 to 0.61050, saving model to Weights_gs531.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.61050\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.61050 to 0.60955, saving model to Weights_gs544.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.60955 to 0.60349, saving model to Weights_gs545.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.60349\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.60349\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.60349 to 0.59927, saving model to Weights_gs548.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.59927 to 0.59844, saving model to Weights_gs549.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.59844\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.59844\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.59844\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.59844 to 0.59580, saving model to Weights_gs553.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.59580\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.59580\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.59580\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.59580\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.59580 to 0.58469, saving model to Weights_gs558.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.58469\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.58469 to 0.58318, saving model to Weights_gs573.h5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.58318 to 0.57680, saving model to Weights_gs574.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.57680\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.57680\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.57680\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.57680 to 0.57514, saving model to Weights_gs578.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.57514\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.57514 to 0.57325, saving model to Weights_gs580.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.57325\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.57325\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.57325 to 0.56429, saving model to Weights_gs583.h5\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.56429\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.56429 to 0.56325, saving model to Weights_gs599.h5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.56325\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.56325 to 0.56150, saving model to Weights_gs5104.h5\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.56150\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.56150 to 0.56077, saving model to Weights_gs5122.h5\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.56077 to 0.55990, saving model to Weights_gs5123.h5\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.55990\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.55990\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.55990\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.55990 to 0.55890, saving model to Weights_gs5127.h5\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.55890\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.55890\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.55890 to 0.55566, saving model to Weights_gs5130.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.55566\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.55566 to 0.55495, saving model to Weights_gs5146.h5\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.55495\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.55495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00162: val_loss improved from 0.55495 to 0.55078, saving model to Weights_gs5162.h5\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.55078\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.55078 to 0.54922, saving model to Weights_gs5177.h5\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.54922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00319: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.54922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00480: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.54922\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21844, saving model to Weights_gs601.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21844 to 0.99403, saving model to Weights_gs602.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.99403 to 0.77929, saving model to Weights_gs603.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.77929\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.77929 to 0.72700, saving model to Weights_gs605.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.72700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.72700\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.72700\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.72700\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.72700 to 0.71730, saving model to Weights_gs610.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.71730\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.71730\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.71730\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.71730\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.71730 to 0.71266, saving model to Weights_gs615.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.71266\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.71266 to 0.67097, saving model to Weights_gs617.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.67097\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.67097\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.67097\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.67097\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.67097 to 0.66046, saving model to Weights_gs622.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.66046\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.66046 to 0.63619, saving model to Weights_gs651.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.63619\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.63619 to 0.63323, saving model to Weights_gs6129.h5\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.63323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00134: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.63323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00293: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.63323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00453: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.63323\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.47481, saving model to Weights_gs701.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.47481 to 0.91785, saving model to Weights_gs702.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.91785 to 0.73669, saving model to Weights_gs703.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.73669 to 0.66622, saving model to Weights_gs704.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.66622\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.66622\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.66622 to 0.62624, saving model to Weights_gs707.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.62624\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.62624\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.62624\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.62624 to 0.60425, saving model to Weights_gs711.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60425\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60425\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60425\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.60425 to 0.59724, saving model to Weights_gs715.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.59724\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.59724\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.59724\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.59724\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.59724\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.59724 to 0.58297, saving model to Weights_gs721.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.58297\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.58297\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.58297\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.58297\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.58297 to 0.57781, saving model to Weights_gs726.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.57781\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.57781 to 0.57389, saving model to Weights_gs728.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.57389\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.57389\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.57389\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.57389\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.57389\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.57389\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.57389\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.57389 to 0.55939, saving model to Weights_gs736.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.55939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00104: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.55939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00263: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.55939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00424: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.55939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 12.77357, saving model to Weights_gs801.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 12.77357 to 12.61235, saving model to Weights_gs802.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 12.61235 to 12.44430, saving model to Weights_gs803.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 12.44430 to 12.27701, saving model to Weights_gs804.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 12.27701 to 12.10563, saving model to Weights_gs805.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 12.10563 to 11.93362, saving model to Weights_gs806.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 11.93362 to 11.75975, saving model to Weights_gs807.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 11.75975 to 11.58444, saving model to Weights_gs808.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 11.58444 to 11.40713, saving model to Weights_gs809.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 11.40713 to 11.22482, saving model to Weights_gs810.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 11.22482 to 11.03174, saving model to Weights_gs811.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 11.03174 to 10.83331, saving model to Weights_gs812.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 10.83331 to 10.62148, saving model to Weights_gs813.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 10.62148 to 10.39466, saving model to Weights_gs814.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 10.39466 to 10.15307, saving model to Weights_gs815.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 10.15307 to 9.90655, saving model to Weights_gs816.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 9.90655 to 9.65092, saving model to Weights_gs817.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 9.65092 to 9.39320, saving model to Weights_gs818.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 9.39320 to 9.12146, saving model to Weights_gs819.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 9.12146 to 8.85089, saving model to Weights_gs820.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 8.85089 to 8.58324, saving model to Weights_gs821.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 8.58324 to 8.31277, saving model to Weights_gs822.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 8.31277 to 8.04420, saving model to Weights_gs823.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 8.04420 to 7.77689, saving model to Weights_gs824.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 7.77689 to 7.50777, saving model to Weights_gs825.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 7.50777 to 7.24172, saving model to Weights_gs826.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 7.24172 to 6.97519, saving model to Weights_gs827.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 6.97519 to 6.71323, saving model to Weights_gs828.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 6.71323 to 6.45697, saving model to Weights_gs829.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 6.45697 to 6.20825, saving model to Weights_gs830.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 6.20825 to 5.96502, saving model to Weights_gs831.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 5.96502 to 5.73259, saving model to Weights_gs832.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 5.73259 to 5.50598, saving model to Weights_gs833.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 5.50598 to 5.28357, saving model to Weights_gs834.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 5.28357 to 5.07325, saving model to Weights_gs835.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 5.07325 to 4.87022, saving model to Weights_gs836.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 4.87022 to 4.67721, saving model to Weights_gs837.h5\n",
      "\n",
      "Epoch 00038: val_loss improved from 4.67721 to 4.49588, saving model to Weights_gs838.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 4.49588 to 4.32526, saving model to Weights_gs839.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 4.32526 to 4.16812, saving model to Weights_gs840.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 4.16812 to 4.02226, saving model to Weights_gs841.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 4.02226 to 3.88060, saving model to Weights_gs842.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 3.88060 to 3.75151, saving model to Weights_gs843.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 3.75151 to 3.63626, saving model to Weights_gs844.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.63626 to 3.53155, saving model to Weights_gs845.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 3.53155 to 3.43013, saving model to Weights_gs846.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.43013 to 3.33748, saving model to Weights_gs847.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 3.33748 to 3.24778, saving model to Weights_gs848.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00049: val_loss improved from 3.24778 to 3.16835, saving model to Weights_gs849.h5\n",
      "\n",
      "Epoch 00050: val_loss improved from 3.16835 to 3.09643, saving model to Weights_gs850.h5\n",
      "\n",
      "Epoch 00051: val_loss improved from 3.09643 to 3.03072, saving model to Weights_gs851.h5\n",
      "\n",
      "Epoch 00052: val_loss improved from 3.03072 to 2.96749, saving model to Weights_gs852.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 2.96749 to 2.90706, saving model to Weights_gs853.h5\n",
      "\n",
      "Epoch 00054: val_loss improved from 2.90706 to 2.85039, saving model to Weights_gs854.h5\n",
      "\n",
      "Epoch 00055: val_loss improved from 2.85039 to 2.79797, saving model to Weights_gs855.h5\n",
      "\n",
      "Epoch 00056: val_loss improved from 2.79797 to 2.74709, saving model to Weights_gs856.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 2.74709 to 2.69551, saving model to Weights_gs857.h5\n",
      "\n",
      "Epoch 00058: val_loss improved from 2.69551 to 2.64765, saving model to Weights_gs858.h5\n",
      "\n",
      "Epoch 00059: val_loss improved from 2.64765 to 2.60179, saving model to Weights_gs859.h5\n",
      "\n",
      "Epoch 00060: val_loss improved from 2.60179 to 2.55704, saving model to Weights_gs860.h5\n",
      "\n",
      "Epoch 00061: val_loss improved from 2.55704 to 2.51535, saving model to Weights_gs861.h5\n",
      "\n",
      "Epoch 00062: val_loss improved from 2.51535 to 2.47430, saving model to Weights_gs862.h5\n",
      "\n",
      "Epoch 00063: val_loss improved from 2.47430 to 2.43443, saving model to Weights_gs863.h5\n",
      "\n",
      "Epoch 00064: val_loss improved from 2.43443 to 2.39506, saving model to Weights_gs864.h5\n",
      "\n",
      "Epoch 00065: val_loss improved from 2.39506 to 2.35655, saving model to Weights_gs865.h5\n",
      "\n",
      "Epoch 00066: val_loss improved from 2.35655 to 2.31740, saving model to Weights_gs866.h5\n",
      "\n",
      "Epoch 00067: val_loss improved from 2.31740 to 2.27979, saving model to Weights_gs867.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 2.27979 to 2.24237, saving model to Weights_gs868.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 2.24237 to 2.20566, saving model to Weights_gs869.h5\n",
      "\n",
      "Epoch 00070: val_loss improved from 2.20566 to 2.17271, saving model to Weights_gs870.h5\n",
      "\n",
      "Epoch 00071: val_loss improved from 2.17271 to 2.13912, saving model to Weights_gs871.h5\n",
      "\n",
      "Epoch 00072: val_loss improved from 2.13912 to 2.10604, saving model to Weights_gs872.h5\n",
      "\n",
      "Epoch 00073: val_loss improved from 2.10604 to 2.07149, saving model to Weights_gs873.h5\n",
      "\n",
      "Epoch 00074: val_loss improved from 2.07149 to 2.03699, saving model to Weights_gs874.h5\n",
      "\n",
      "Epoch 00075: val_loss improved from 2.03699 to 2.00487, saving model to Weights_gs875.h5\n",
      "\n",
      "Epoch 00076: val_loss improved from 2.00487 to 1.97262, saving model to Weights_gs876.h5\n",
      "\n",
      "Epoch 00077: val_loss improved from 1.97262 to 1.94134, saving model to Weights_gs877.h5\n",
      "\n",
      "Epoch 00078: val_loss improved from 1.94134 to 1.91174, saving model to Weights_gs878.h5\n",
      "\n",
      "Epoch 00079: val_loss improved from 1.91174 to 1.88095, saving model to Weights_gs879.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 1.88095 to 1.85145, saving model to Weights_gs880.h5\n",
      "\n",
      "Epoch 00081: val_loss improved from 1.85145 to 1.82351, saving model to Weights_gs881.h5\n",
      "\n",
      "Epoch 00082: val_loss improved from 1.82351 to 1.79396, saving model to Weights_gs882.h5\n",
      "\n",
      "Epoch 00083: val_loss improved from 1.79396 to 1.76578, saving model to Weights_gs883.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 1.76578 to 1.73881, saving model to Weights_gs884.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 1.73881 to 1.71123, saving model to Weights_gs885.h5\n",
      "\n",
      "Epoch 00086: val_loss improved from 1.71123 to 1.68555, saving model to Weights_gs886.h5\n",
      "\n",
      "Epoch 00087: val_loss improved from 1.68555 to 1.65933, saving model to Weights_gs887.h5\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.65933 to 1.63346, saving model to Weights_gs888.h5\n",
      "\n",
      "Epoch 00089: val_loss improved from 1.63346 to 1.60755, saving model to Weights_gs889.h5\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.60755 to 1.58155, saving model to Weights_gs890.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 1.58155 to 1.55544, saving model to Weights_gs891.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 1.55544 to 1.53078, saving model to Weights_gs892.h5\n",
      "\n",
      "Epoch 00093: val_loss improved from 1.53078 to 1.50687, saving model to Weights_gs893.h5\n",
      "\n",
      "Epoch 00094: val_loss improved from 1.50687 to 1.48456, saving model to Weights_gs894.h5\n",
      "\n",
      "Epoch 00095: val_loss improved from 1.48456 to 1.46095, saving model to Weights_gs895.h5\n",
      "\n",
      "Epoch 00096: val_loss improved from 1.46095 to 1.43880, saving model to Weights_gs896.h5\n",
      "\n",
      "Epoch 00097: val_loss improved from 1.43880 to 1.41558, saving model to Weights_gs897.h5\n",
      "\n",
      "Epoch 00098: val_loss improved from 1.41558 to 1.39278, saving model to Weights_gs898.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 1.39278 to 1.37182, saving model to Weights_gs899.h5\n",
      "\n",
      "Epoch 00100: val_loss improved from 1.37182 to 1.35181, saving model to Weights_gs8100.h5\n",
      "\n",
      "Epoch 00101: val_loss improved from 1.35181 to 1.33199, saving model to Weights_gs8101.h5\n",
      "\n",
      "Epoch 00102: val_loss improved from 1.33199 to 1.31327, saving model to Weights_gs8102.h5\n",
      "\n",
      "Epoch 00103: val_loss improved from 1.31327 to 1.29415, saving model to Weights_gs8103.h5\n",
      "\n",
      "Epoch 00104: val_loss improved from 1.29415 to 1.27522, saving model to Weights_gs8104.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 1.27522 to 1.25656, saving model to Weights_gs8105.h5\n",
      "\n",
      "Epoch 00106: val_loss improved from 1.25656 to 1.23769, saving model to Weights_gs8106.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 1.23769 to 1.21896, saving model to Weights_gs8107.h5\n",
      "\n",
      "Epoch 00108: val_loss improved from 1.21896 to 1.20142, saving model to Weights_gs8108.h5\n",
      "\n",
      "Epoch 00109: val_loss improved from 1.20142 to 1.18542, saving model to Weights_gs8109.h5\n",
      "\n",
      "Epoch 00110: val_loss improved from 1.18542 to 1.16925, saving model to Weights_gs8110.h5\n",
      "\n",
      "Epoch 00111: val_loss improved from 1.16925 to 1.15336, saving model to Weights_gs8111.h5\n",
      "\n",
      "Epoch 00112: val_loss improved from 1.15336 to 1.13735, saving model to Weights_gs8112.h5\n",
      "\n",
      "Epoch 00113: val_loss improved from 1.13735 to 1.12112, saving model to Weights_gs8113.h5\n",
      "\n",
      "Epoch 00114: val_loss improved from 1.12112 to 1.10708, saving model to Weights_gs8114.h5\n",
      "\n",
      "Epoch 00115: val_loss improved from 1.10708 to 1.09329, saving model to Weights_gs8115.h5\n",
      "\n",
      "Epoch 00116: val_loss improved from 1.09329 to 1.07908, saving model to Weights_gs8116.h5\n",
      "\n",
      "Epoch 00117: val_loss improved from 1.07908 to 1.06677, saving model to Weights_gs8117.h5\n",
      "\n",
      "Epoch 00118: val_loss improved from 1.06677 to 1.05381, saving model to Weights_gs8118.h5\n",
      "\n",
      "Epoch 00119: val_loss improved from 1.05381 to 1.04147, saving model to Weights_gs8119.h5\n",
      "\n",
      "Epoch 00120: val_loss improved from 1.04147 to 1.03000, saving model to Weights_gs8120.h5\n",
      "\n",
      "Epoch 00121: val_loss improved from 1.03000 to 1.01908, saving model to Weights_gs8121.h5\n",
      "\n",
      "Epoch 00122: val_loss improved from 1.01908 to 1.00695, saving model to Weights_gs8122.h5\n",
      "\n",
      "Epoch 00123: val_loss improved from 1.00695 to 0.99550, saving model to Weights_gs8123.h5\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.99550 to 0.98506, saving model to Weights_gs8124.h5\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.98506 to 0.97563, saving model to Weights_gs8125.h5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.97563 to 0.96607, saving model to Weights_gs8126.h5\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.96607 to 0.95792, saving model to Weights_gs8127.h5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.95792 to 0.94974, saving model to Weights_gs8128.h5\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.94974 to 0.94326, saving model to Weights_gs8129.h5\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.94326 to 0.93491, saving model to Weights_gs8130.h5\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.93491 to 0.92649, saving model to Weights_gs8131.h5\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.92649 to 0.91862, saving model to Weights_gs8132.h5\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.91862 to 0.91105, saving model to Weights_gs8133.h5\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.91105 to 0.90420, saving model to Weights_gs8134.h5\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.90420 to 0.89778, saving model to Weights_gs8135.h5\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.89778 to 0.89175, saving model to Weights_gs8136.h5\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.89175 to 0.88513, saving model to Weights_gs8137.h5\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.88513 to 0.87807, saving model to Weights_gs8138.h5\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.87807 to 0.87312, saving model to Weights_gs8139.h5\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.87312 to 0.86805, saving model to Weights_gs8140.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00141: val_loss improved from 0.86805 to 0.86331, saving model to Weights_gs8141.h5\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.86331 to 0.85925, saving model to Weights_gs8142.h5\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.85925 to 0.85438, saving model to Weights_gs8143.h5\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.85438 to 0.84868, saving model to Weights_gs8144.h5\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.84868 to 0.84462, saving model to Weights_gs8145.h5\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.84462 to 0.84041, saving model to Weights_gs8146.h5\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.84041 to 0.83703, saving model to Weights_gs8147.h5\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.83703 to 0.83355, saving model to Weights_gs8148.h5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.83355 to 0.83076, saving model to Weights_gs8149.h5\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.83076 to 0.82820, saving model to Weights_gs8150.h5\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.82820 to 0.82403, saving model to Weights_gs8151.h5\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.82403 to 0.82143, saving model to Weights_gs8152.h5\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.82143 to 0.81701, saving model to Weights_gs8153.h5\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.81701 to 0.81287, saving model to Weights_gs8154.h5\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.81287 to 0.80951, saving model to Weights_gs8155.h5\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.80951 to 0.80600, saving model to Weights_gs8156.h5\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.80600 to 0.80293, saving model to Weights_gs8157.h5\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.80293 to 0.79997, saving model to Weights_gs8158.h5\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.79997 to 0.79689, saving model to Weights_gs8159.h5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.79689 to 0.79417, saving model to Weights_gs8160.h5\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.79417 to 0.79220, saving model to Weights_gs8161.h5\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.79220 to 0.78939, saving model to Weights_gs8162.h5\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.78939 to 0.78709, saving model to Weights_gs8163.h5\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.78709 to 0.78468, saving model to Weights_gs8164.h5\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.78468 to 0.78167, saving model to Weights_gs8165.h5\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.78167 to 0.77903, saving model to Weights_gs8166.h5\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.77903 to 0.77675, saving model to Weights_gs8167.h5\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.77675 to 0.77360, saving model to Weights_gs8168.h5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.77360 to 0.77084, saving model to Weights_gs8169.h5\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.77084 to 0.76927, saving model to Weights_gs8170.h5\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.76927 to 0.76746, saving model to Weights_gs8171.h5\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.76746 to 0.76412, saving model to Weights_gs8172.h5\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.76412 to 0.76185, saving model to Weights_gs8173.h5\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.76185 to 0.76032, saving model to Weights_gs8174.h5\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.76032 to 0.75873, saving model to Weights_gs8175.h5\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.75873 to 0.75694, saving model to Weights_gs8176.h5\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.75694 to 0.75514, saving model to Weights_gs8177.h5\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.75514 to 0.75293, saving model to Weights_gs8178.h5\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.75293 to 0.75151, saving model to Weights_gs8179.h5\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.75151 to 0.74989, saving model to Weights_gs8180.h5\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.74989 to 0.74785, saving model to Weights_gs8181.h5\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.74785 to 0.74479, saving model to Weights_gs8182.h5\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.74479 to 0.74282, saving model to Weights_gs8183.h5\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.74282 to 0.74259, saving model to Weights_gs8184.h5\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.74259 to 0.74167, saving model to Weights_gs8185.h5\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.74167 to 0.73966, saving model to Weights_gs8186.h5\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.73966 to 0.73937, saving model to Weights_gs8187.h5\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.73937 to 0.73873, saving model to Weights_gs8188.h5\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.73873 to 0.73567, saving model to Weights_gs8189.h5\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.73567 to 0.73382, saving model to Weights_gs8190.h5\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.73382 to 0.73315, saving model to Weights_gs8191.h5\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.73315 to 0.73242, saving model to Weights_gs8192.h5\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.73242 to 0.72963, saving model to Weights_gs8193.h5\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.72963 to 0.72779, saving model to Weights_gs8194.h5\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.72779 to 0.72691, saving model to Weights_gs8195.h5\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.72691 to 0.72671, saving model to Weights_gs8196.h5\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.72671 to 0.72491, saving model to Weights_gs8197.h5\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.72491 to 0.72258, saving model to Weights_gs8198.h5\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.72258 to 0.72102, saving model to Weights_gs8199.h5\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.72102 to 0.71880, saving model to Weights_gs8200.h5\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.71880 to 0.71725, saving model to Weights_gs8201.h5\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.71725 to 0.71482, saving model to Weights_gs8202.h5\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.71482 to 0.71304, saving model to Weights_gs8203.h5\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.71304 to 0.71189, saving model to Weights_gs8204.h5\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.71189 to 0.71054, saving model to Weights_gs8205.h5\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.71054 to 0.70884, saving model to Weights_gs8206.h5\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.70884 to 0.70763, saving model to Weights_gs8207.h5\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.70763 to 0.70667, saving model to Weights_gs8208.h5\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.70667 to 0.70556, saving model to Weights_gs8209.h5\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.70556 to 0.70434, saving model to Weights_gs8210.h5\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.70434 to 0.70335, saving model to Weights_gs8211.h5\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.70335 to 0.70225, saving model to Weights_gs8212.h5\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.70225 to 0.70046, saving model to Weights_gs8213.h5\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.70046 to 0.69998, saving model to Weights_gs8214.h5\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.69998 to 0.69880, saving model to Weights_gs8215.h5\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.69880 to 0.69778, saving model to Weights_gs8216.h5\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.69778 to 0.69687, saving model to Weights_gs8217.h5\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.69687 to 0.69661, saving model to Weights_gs8218.h5\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.69661 to 0.69526, saving model to Weights_gs8219.h5\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.69526 to 0.69438, saving model to Weights_gs8220.h5\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.69438 to 0.69322, saving model to Weights_gs8221.h5\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.69322\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.69322 to 0.69234, saving model to Weights_gs8223.h5\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.69234 to 0.69152, saving model to Weights_gs8224.h5\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.69152 to 0.68980, saving model to Weights_gs8225.h5\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.68980 to 0.68943, saving model to Weights_gs8226.h5\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.68943 to 0.68923, saving model to Weights_gs8227.h5\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.68923 to 0.68840, saving model to Weights_gs8228.h5\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.68840 to 0.68763, saving model to Weights_gs8229.h5\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.68763 to 0.68742, saving model to Weights_gs8230.h5\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.68742 to 0.68693, saving model to Weights_gs8231.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00232: val_loss improved from 0.68693 to 0.68507, saving model to Weights_gs8232.h5\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.68507 to 0.68440, saving model to Weights_gs8233.h5\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.68440 to 0.68357, saving model to Weights_gs8234.h5\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.68357 to 0.68232, saving model to Weights_gs8235.h5\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.68232 to 0.68194, saving model to Weights_gs8236.h5\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.68194 to 0.68094, saving model to Weights_gs8237.h5\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.68094 to 0.67954, saving model to Weights_gs8238.h5\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.67954 to 0.67831, saving model to Weights_gs8239.h5\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.67831 to 0.67738, saving model to Weights_gs8240.h5\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.67738 to 0.67653, saving model to Weights_gs8241.h5\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.67653 to 0.67649, saving model to Weights_gs8242.h5\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.67649 to 0.67642, saving model to Weights_gs8243.h5\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.67642 to 0.67602, saving model to Weights_gs8244.h5\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.67602 to 0.67599, saving model to Weights_gs8245.h5\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.67599 to 0.67501, saving model to Weights_gs8246.h5\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.67501 to 0.67398, saving model to Weights_gs8247.h5\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.67398 to 0.67206, saving model to Weights_gs8248.h5\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.67206 to 0.67051, saving model to Weights_gs8249.h5\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.67051 to 0.66903, saving model to Weights_gs8250.h5\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.66903 to 0.66760, saving model to Weights_gs8251.h5\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.66760 to 0.66667, saving model to Weights_gs8252.h5\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.66667 to 0.66532, saving model to Weights_gs8253.h5\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.66532 to 0.66509, saving model to Weights_gs8254.h5\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.66509 to 0.66473, saving model to Weights_gs8255.h5\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.66473 to 0.66460, saving model to Weights_gs8256.h5\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.66460 to 0.66439, saving model to Weights_gs8257.h5\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.66439 to 0.66333, saving model to Weights_gs8258.h5\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.66333 to 0.66243, saving model to Weights_gs8259.h5\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.66243 to 0.66204, saving model to Weights_gs8260.h5\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.66204 to 0.66144, saving model to Weights_gs8261.h5\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.66144 to 0.66115, saving model to Weights_gs8262.h5\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.66115 to 0.66030, saving model to Weights_gs8263.h5\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.66030\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.66030\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.66030\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.66030\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.66030\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.66030\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.66030 to 0.65970, saving model to Weights_gs8270.h5\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.65970 to 0.65967, saving model to Weights_gs8271.h5\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.65967 to 0.65761, saving model to Weights_gs8272.h5\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.65761 to 0.65684, saving model to Weights_gs8273.h5\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.65684 to 0.65629, saving model to Weights_gs8274.h5\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.65629 to 0.65591, saving model to Weights_gs8275.h5\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.65591 to 0.65541, saving model to Weights_gs8276.h5\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.65541 to 0.65437, saving model to Weights_gs8277.h5\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.65437 to 0.65360, saving model to Weights_gs8278.h5\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.65360 to 0.65319, saving model to Weights_gs8279.h5\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.65319 to 0.65263, saving model to Weights_gs8280.h5\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.65263 to 0.65167, saving model to Weights_gs8281.h5\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.65167\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.65167 to 0.65154, saving model to Weights_gs8283.h5\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.65154\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.65154 to 0.65109, saving model to Weights_gs8285.h5\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.65109 to 0.65011, saving model to Weights_gs8286.h5\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.65011 to 0.64995, saving model to Weights_gs8287.h5\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.64995\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.64995\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.64995\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.64995\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.64995\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.64995\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.64995 to 0.64926, saving model to Weights_gs8294.h5\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.64926\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.64926 to 0.64853, saving model to Weights_gs8296.h5\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.64853 to 0.64738, saving model to Weights_gs8297.h5\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.64738 to 0.64656, saving model to Weights_gs8298.h5\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.64656 to 0.64589, saving model to Weights_gs8299.h5\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.64589 to 0.64517, saving model to Weights_gs8300.h5\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.64517 to 0.64438, saving model to Weights_gs8301.h5\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.64438 to 0.64407, saving model to Weights_gs8302.h5\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.64407 to 0.64395, saving model to Weights_gs8303.h5\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.64395 to 0.64299, saving model to Weights_gs8304.h5\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.64299\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.64299 to 0.64288, saving model to Weights_gs8306.h5\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.64288\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.64288\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.64288\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.64288 to 0.64278, saving model to Weights_gs8310.h5\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.64278\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.64278 to 0.64255, saving model to Weights_gs8312.h5\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.64255 to 0.64217, saving model to Weights_gs8313.h5\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.64217 to 0.64215, saving model to Weights_gs8314.h5\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.64215\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.64215\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.64215 to 0.64169, saving model to Weights_gs8317.h5\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.64169\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.64169\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.64169 to 0.64100, saving model to Weights_gs8320.h5\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.64100 to 0.64045, saving model to Weights_gs8321.h5\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.64045\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.64045 to 0.64039, saving model to Weights_gs8323.h5\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.64039 to 0.64033, saving model to Weights_gs8324.h5\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.64033 to 0.63981, saving model to Weights_gs8325.h5\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.63981 to 0.63824, saving model to Weights_gs8326.h5\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.63824\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.63824 to 0.63713, saving model to Weights_gs8328.h5\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.63713 to 0.63634, saving model to Weights_gs8329.h5\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.63634 to 0.63608, saving model to Weights_gs8330.h5\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.63608\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.63608\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.63608\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.63608\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.63608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00336: val_loss improved from 0.63608 to 0.63585, saving model to Weights_gs8336.h5\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.63585 to 0.63569, saving model to Weights_gs8337.h5\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.63569 to 0.63523, saving model to Weights_gs8338.h5\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.63523\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.63523 to 0.63461, saving model to Weights_gs8352.h5\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.63461\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.63461 to 0.63342, saving model to Weights_gs8354.h5\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.63342 to 0.63221, saving model to Weights_gs8355.h5\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.63221 to 0.63173, saving model to Weights_gs8356.h5\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.63173\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.63173 to 0.63161, saving model to Weights_gs8358.h5\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.63161 to 0.63132, saving model to Weights_gs8359.h5\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.63132 to 0.63011, saving model to Weights_gs8360.h5\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.63011 to 0.62953, saving model to Weights_gs8361.h5\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.62953 to 0.62924, saving model to Weights_gs8362.h5\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.62924\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.62924 to 0.62922, saving model to Weights_gs8364.h5\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.62922 to 0.62881, saving model to Weights_gs8365.h5\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.62881 to 0.62841, saving model to Weights_gs8366.h5\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.62841\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.62841\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.62841\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.62841\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.62841 to 0.62755, saving model to Weights_gs8371.h5\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.62755 to 0.62744, saving model to Weights_gs8372.h5\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.62744\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.62744 to 0.62737, saving model to Weights_gs8374.h5\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.62737 to 0.62711, saving model to Weights_gs8375.h5\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.62711\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.62711\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.62711 to 0.62599, saving model to Weights_gs8378.h5\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.62599 to 0.62499, saving model to Weights_gs8379.h5\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.62499 to 0.62438, saving model to Weights_gs8380.h5\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.62438\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.62438\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.62438\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.62438\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.62438\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.62438\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.62438\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.62438 to 0.62433, saving model to Weights_gs8388.h5\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.62433\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.62433 to 0.62378, saving model to Weights_gs8390.h5\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.62378 to 0.62377, saving model to Weights_gs8391.h5\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.62377\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.62377\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.62377 to 0.62359, saving model to Weights_gs8394.h5\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.62359 to 0.62284, saving model to Weights_gs8395.h5\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.62284 to 0.62221, saving model to Weights_gs8396.h5\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.62221 to 0.62214, saving model to Weights_gs8397.h5\n",
      "\n",
      "Epoch 00398: val_loss improved from 0.62214 to 0.62169, saving model to Weights_gs8398.h5\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.62169 to 0.62122, saving model to Weights_gs8399.h5\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.62122 to 0.62110, saving model to Weights_gs8400.h5\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.62110\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.62110\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.62110\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.62110 to 0.62050, saving model to Weights_gs8404.h5\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.62050\n",
      "\n",
      "Epoch 00431: val_loss improved from 0.62050 to 0.62021, saving model to Weights_gs8431.h5\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.62021 to 0.61976, saving model to Weights_gs8432.h5\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.61976 to 0.61970, saving model to Weights_gs8433.h5\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.61970\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.61970 to 0.61937, saving model to Weights_gs8435.h5\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.61937\n",
      "\n",
      "Epoch 00437: val_loss improved from 0.61937 to 0.61872, saving model to Weights_gs8437.h5\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.61872\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.61872\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.61872 to 0.61866, saving model to Weights_gs8440.h5\n",
      "\n",
      "Epoch 00441: val_loss improved from 0.61866 to 0.61803, saving model to Weights_gs8441.h5\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.61803\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.61803\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.61803\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.61803\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.61803 to 0.61793, saving model to Weights_gs8446.h5\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.61793 to 0.61767, saving model to Weights_gs8447.h5\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.61767 to 0.61752, saving model to Weights_gs8448.h5\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.61752\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.61752\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.61752\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.61752\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.61752\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.61752\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.61752 to 0.61733, saving model to Weights_gs8455.h5\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.61733\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.61733 to 0.61704, saving model to Weights_gs8457.h5\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.61704\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.61704 to 0.61615, saving model to Weights_gs8459.h5\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.61615\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.61615\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.61615 to 0.61565, saving model to Weights_gs8462.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00463: val_loss improved from 0.61565 to 0.61484, saving model to Weights_gs8463.h5\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.61484 to 0.61431, saving model to Weights_gs8464.h5\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.61431 to 0.61409, saving model to Weights_gs8465.h5\n",
      "\n",
      "Epoch 00466: val_loss improved from 0.61409 to 0.61363, saving model to Weights_gs8466.h5\n",
      "\n",
      "Epoch 00467: val_loss improved from 0.61363 to 0.61326, saving model to Weights_gs8467.h5\n",
      "\n",
      "Epoch 00468: val_loss improved from 0.61326 to 0.61326, saving model to Weights_gs8468.h5\n",
      "\n",
      "Epoch 00469: val_loss improved from 0.61326 to 0.61274, saving model to Weights_gs8469.h5\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.61274\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.61274 to 0.61204, saving model to Weights_gs8471.h5\n",
      "\n",
      "Epoch 00472: val_loss improved from 0.61204 to 0.61089, saving model to Weights_gs8472.h5\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.61089 to 0.60997, saving model to Weights_gs8473.h5\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.60997\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.14614, saving model to Weights_gs901.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.14614 to 7.68914, saving model to Weights_gs902.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 7.68914 to 5.04621, saving model to Weights_gs903.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.04621 to 3.34929, saving model to Weights_gs904.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.34929 to 2.75797, saving model to Weights_gs905.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.75797 to 2.32606, saving model to Weights_gs906.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.32606 to 1.97216, saving model to Weights_gs907.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.97216 to 1.69837, saving model to Weights_gs908.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.69837 to 1.49274, saving model to Weights_gs909.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.49274 to 1.32152, saving model to Weights_gs910.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.32152 to 1.19174, saving model to Weights_gs911.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.19174 to 1.09639, saving model to Weights_gs912.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.09639 to 1.02209, saving model to Weights_gs913.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.02209 to 0.95994, saving model to Weights_gs914.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.95994 to 0.90981, saving model to Weights_gs915.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.90981 to 0.86782, saving model to Weights_gs916.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.86782 to 0.83287, saving model to Weights_gs917.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.83287 to 0.81318, saving model to Weights_gs918.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.81318 to 0.79333, saving model to Weights_gs919.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.79333 to 0.77548, saving model to Weights_gs920.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.77548 to 0.75717, saving model to Weights_gs921.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.75717 to 0.73565, saving model to Weights_gs922.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.73565 to 0.71728, saving model to Weights_gs923.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.71728 to 0.70470, saving model to Weights_gs924.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.70470 to 0.70037, saving model to Weights_gs925.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.70037 to 0.68995, saving model to Weights_gs926.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.68995 to 0.67972, saving model to Weights_gs927.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.67972 to 0.67176, saving model to Weights_gs928.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.67176 to 0.66960, saving model to Weights_gs929.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66960\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.66960 to 0.66192, saving model to Weights_gs931.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.66192 to 0.65707, saving model to Weights_gs932.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.65707 to 0.64717, saving model to Weights_gs933.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.64717 to 0.63791, saving model to Weights_gs934.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.63791\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.63791 to 0.63648, saving model to Weights_gs936.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.63648 to 0.63205, saving model to Weights_gs937.h5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.63205 to 0.63051, saving model to Weights_gs938.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.63051 to 0.62713, saving model to Weights_gs939.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.62713\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.62713\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.62713\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.62713 to 0.61987, saving model to Weights_gs943.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.61987 to 0.61754, saving model to Weights_gs944.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.61754 to 0.61435, saving model to Weights_gs945.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.61435 to 0.60888, saving model to Weights_gs946.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.60888 to 0.60382, saving model to Weights_gs947.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.60382\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.60382\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.60382\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.60382\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.60382\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.60382 to 0.59573, saving model to Weights_gs953.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.59573\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.59573 to 0.59107, saving model to Weights_gs966.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.59107\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.59107\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.59107 to 0.59008, saving model to Weights_gs969.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.59008\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.59008 to 0.58931, saving model to Weights_gs971.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.58931\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.58931 to 0.58328, saving model to Weights_gs981.h5\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.58328 to 0.57448, saving model to Weights_gs982.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.57448\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.57448 to 0.57424, saving model to Weights_gs9101.h5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.57424\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.57424 to 0.57288, saving model to Weights_gs9112.h5\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.57288\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.57288\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.57288\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.57288 to 0.57283, saving model to Weights_gs9116.h5\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.57283\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.57283 to 0.57232, saving model to Weights_gs9118.h5\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.57232\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.57232 to 0.56868, saving model to Weights_gs9137.h5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.56868\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.56868 to 0.56771, saving model to Weights_gs9139.h5\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.56771 to 0.56736, saving model to Weights_gs9140.h5\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.56736\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.56736\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.56736\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.56736\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.56736\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.56736\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.56736 to 0.56455, saving model to Weights_gs9147.h5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.56455\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.56455\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.56455\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.56455\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.56455\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.56455\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.56455\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.56455 to 0.56247, saving model to Weights_gs9155.h5\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.56247 to 0.56019, saving model to Weights_gs9156.h5\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.56019\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.56019 to 0.55827, saving model to Weights_gs9168.h5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.55827 to 0.55609, saving model to Weights_gs9169.h5\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.55609 to 0.55517, saving model to Weights_gs9170.h5\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.55517\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.55517 to 0.55430, saving model to Weights_gs9172.h5\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.55430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00237: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.55430\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.55430 to 0.55397, saving model to Weights_gs9241.h5\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.55397\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.55397\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.55397\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.55397\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.55397\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.55397\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.55397 to 0.54679, saving model to Weights_gs9248.h5\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.54679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00395: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.54679\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.38585, saving model to Weights_gs1001.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.38585 to 1.74247, saving model to Weights_gs1002.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.74247 to 1.28411, saving model to Weights_gs1003.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.28411 to 1.03451, saving model to Weights_gs1004.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.03451\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.03451\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.03451 to 0.88945, saving model to Weights_gs1007.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.88945 to 0.82116, saving model to Weights_gs1008.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.82116\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.82116\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.82116\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.82116 to 0.72632, saving model to Weights_gs1012.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.72632 to 0.71672, saving model to Weights_gs1013.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.71672\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.71672 to 0.71504, saving model to Weights_gs1015.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.71504\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.71504 to 0.68168, saving model to Weights_gs1017.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.68168\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.68168 to 0.63042, saving model to Weights_gs1019.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.63042\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.63042 to 0.55379, saving model to Weights_gs1031.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.55379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.55379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00211: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.55379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00369: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.55379\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77061, saving model to Weights_gs1101.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.77061 to 1.16937, saving model to Weights_gs1102.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.16937 to 0.82947, saving model to Weights_gs1103.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.82947 to 0.75650, saving model to Weights_gs1104.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.75650 to 0.71915, saving model to Weights_gs1105.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.71915 to 0.69140, saving model to Weights_gs1106.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.69140 to 0.62822, saving model to Weights_gs1107.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.62822 to 0.62270, saving model to Weights_gs1108.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.62270 to 0.60895, saving model to Weights_gs1109.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.60895 to 0.59663, saving model to Weights_gs1110.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.59663\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.59663 to 0.58983, saving model to Weights_gs1112.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.58983 to 0.58205, saving model to Weights_gs1113.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.58205 to 0.57311, saving model to Weights_gs1114.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.57311\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.57311 to 0.55175, saving model to Weights_gs1116.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: val_loss did not improve from 0.55175\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.55175\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.55175\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.55175\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.55175\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.55175\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.55175 to 0.53380, saving model to Weights_gs1123.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53380\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.53380\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.53380\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.53380\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.53380 to 0.52873, saving model to Weights_gs1128.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.52873\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.52873\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.52873\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.52873\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.52873 to 0.51929, saving model to Weights_gs1133.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.51929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00177: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.51929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00339: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.51929\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.51929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 13.23351, saving model to Weights_gs1201.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 13.23351 to 13.14031, saving model to Weights_gs1202.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 13.14031 to 13.04923, saving model to Weights_gs1203.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 13.04923 to 12.96145, saving model to Weights_gs1204.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 12.96145 to 12.87544, saving model to Weights_gs1205.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 12.87544 to 12.79042, saving model to Weights_gs1206.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 12.79042 to 12.70134, saving model to Weights_gs1207.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 12.70134 to 12.60854, saving model to Weights_gs1208.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 12.60854 to 12.51197, saving model to Weights_gs1209.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 12.51197 to 12.41339, saving model to Weights_gs1210.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 12.41339 to 12.31114, saving model to Weights_gs1211.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 12.31114 to 12.20540, saving model to Weights_gs1212.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 12.20540 to 12.09363, saving model to Weights_gs1213.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 12.09363 to 11.97953, saving model to Weights_gs1214.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 11.97953 to 11.85996, saving model to Weights_gs1215.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 11.85996 to 11.74042, saving model to Weights_gs1216.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 11.74042 to 11.62342, saving model to Weights_gs1217.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 11.62342 to 11.50594, saving model to Weights_gs1218.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 11.50594 to 11.38655, saving model to Weights_gs1219.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 11.38655 to 11.26859, saving model to Weights_gs1220.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 11.26859 to 11.15270, saving model to Weights_gs1221.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 11.15270 to 11.03472, saving model to Weights_gs1222.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 11.03472 to 10.91785, saving model to Weights_gs1223.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 10.91785 to 10.79975, saving model to Weights_gs1224.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 10.79975 to 10.67968, saving model to Weights_gs1225.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 10.67968 to 10.55851, saving model to Weights_gs1226.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 10.55851 to 10.43621, saving model to Weights_gs1227.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 10.43621 to 10.31440, saving model to Weights_gs1228.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 10.31440 to 10.19590, saving model to Weights_gs1229.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 10.19590 to 10.07813, saving model to Weights_gs1230.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 10.07813 to 9.95905, saving model to Weights_gs1231.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 9.95905 to 9.83958, saving model to Weights_gs1232.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 9.83958 to 9.71845, saving model to Weights_gs1233.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 9.71845 to 9.59691, saving model to Weights_gs1234.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 9.59691 to 9.47348, saving model to Weights_gs1235.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 9.47348 to 9.35107, saving model to Weights_gs1236.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 9.35107 to 9.22470, saving model to Weights_gs1237.h5\n",
      "\n",
      "Epoch 00038: val_loss improved from 9.22470 to 9.10208, saving model to Weights_gs1238.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 9.10208 to 8.97941, saving model to Weights_gs1239.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 8.97941 to 8.85588, saving model to Weights_gs1240.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 8.85588 to 8.73123, saving model to Weights_gs1241.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 8.73123 to 8.60508, saving model to Weights_gs1242.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 8.60508 to 8.48213, saving model to Weights_gs1243.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 8.48213 to 8.35538, saving model to Weights_gs1244.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 8.35538 to 8.22840, saving model to Weights_gs1245.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 8.22840 to 8.10120, saving model to Weights_gs1246.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 8.10120 to 7.97535, saving model to Weights_gs1247.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 7.97535 to 7.85003, saving model to Weights_gs1248.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 7.85003 to 7.72832, saving model to Weights_gs1249.h5\n",
      "\n",
      "Epoch 00050: val_loss improved from 7.72832 to 7.60471, saving model to Weights_gs1250.h5\n",
      "\n",
      "Epoch 00051: val_loss improved from 7.60471 to 7.48251, saving model to Weights_gs1251.h5\n",
      "\n",
      "Epoch 00052: val_loss improved from 7.48251 to 7.36032, saving model to Weights_gs1252.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 7.36032 to 7.24223, saving model to Weights_gs1253.h5\n",
      "\n",
      "Epoch 00054: val_loss improved from 7.24223 to 7.12274, saving model to Weights_gs1254.h5\n",
      "\n",
      "Epoch 00055: val_loss improved from 7.12274 to 7.00262, saving model to Weights_gs1255.h5\n",
      "\n",
      "Epoch 00056: val_loss improved from 7.00262 to 6.88315, saving model to Weights_gs1256.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 6.88315 to 6.75922, saving model to Weights_gs1257.h5\n",
      "\n",
      "Epoch 00058: val_loss improved from 6.75922 to 6.63961, saving model to Weights_gs1258.h5\n",
      "\n",
      "Epoch 00059: val_loss improved from 6.63961 to 6.52101, saving model to Weights_gs1259.h5\n",
      "\n",
      "Epoch 00060: val_loss improved from 6.52101 to 6.40763, saving model to Weights_gs1260.h5\n",
      "\n",
      "Epoch 00061: val_loss improved from 6.40763 to 6.29516, saving model to Weights_gs1261.h5\n",
      "\n",
      "Epoch 00062: val_loss improved from 6.29516 to 6.18332, saving model to Weights_gs1262.h5\n",
      "\n",
      "Epoch 00063: val_loss improved from 6.18332 to 6.07433, saving model to Weights_gs1263.h5\n",
      "\n",
      "Epoch 00064: val_loss improved from 6.07433 to 5.96240, saving model to Weights_gs1264.h5\n",
      "\n",
      "Epoch 00065: val_loss improved from 5.96240 to 5.85247, saving model to Weights_gs1265.h5\n",
      "\n",
      "Epoch 00066: val_loss improved from 5.85247 to 5.74528, saving model to Weights_gs1266.h5\n",
      "\n",
      "Epoch 00067: val_loss improved from 5.74528 to 5.64012, saving model to Weights_gs1267.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 5.64012 to 5.54038, saving model to Weights_gs1268.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 5.54038 to 5.44144, saving model to Weights_gs1269.h5\n",
      "\n",
      "Epoch 00070: val_loss improved from 5.44144 to 5.34323, saving model to Weights_gs1270.h5\n",
      "\n",
      "Epoch 00071: val_loss improved from 5.34323 to 5.24728, saving model to Weights_gs1271.h5\n",
      "\n",
      "Epoch 00072: val_loss improved from 5.24728 to 5.15313, saving model to Weights_gs1272.h5\n",
      "\n",
      "Epoch 00073: val_loss improved from 5.15313 to 5.06245, saving model to Weights_gs1273.h5\n",
      "\n",
      "Epoch 00074: val_loss improved from 5.06245 to 4.97065, saving model to Weights_gs1274.h5\n",
      "\n",
      "Epoch 00075: val_loss improved from 4.97065 to 4.88282, saving model to Weights_gs1275.h5\n",
      "\n",
      "Epoch 00076: val_loss improved from 4.88282 to 4.79575, saving model to Weights_gs1276.h5\n",
      "\n",
      "Epoch 00077: val_loss improved from 4.79575 to 4.70930, saving model to Weights_gs1277.h5\n",
      "\n",
      "Epoch 00078: val_loss improved from 4.70930 to 4.62450, saving model to Weights_gs1278.h5\n",
      "\n",
      "Epoch 00079: val_loss improved from 4.62450 to 4.54161, saving model to Weights_gs1279.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 4.54161 to 4.46308, saving model to Weights_gs1280.h5\n",
      "\n",
      "Epoch 00081: val_loss improved from 4.46308 to 4.38714, saving model to Weights_gs1281.h5\n",
      "\n",
      "Epoch 00082: val_loss improved from 4.38714 to 4.31271, saving model to Weights_gs1282.h5\n",
      "\n",
      "Epoch 00083: val_loss improved from 4.31271 to 4.24129, saving model to Weights_gs1283.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 4.24129 to 4.17246, saving model to Weights_gs1284.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 4.17246 to 4.10268, saving model to Weights_gs1285.h5\n",
      "\n",
      "Epoch 00086: val_loss improved from 4.10268 to 4.03729, saving model to Weights_gs1286.h5\n",
      "\n",
      "Epoch 00087: val_loss improved from 4.03729 to 3.97232, saving model to Weights_gs1287.h5\n",
      "\n",
      "Epoch 00088: val_loss improved from 3.97232 to 3.90826, saving model to Weights_gs1288.h5\n",
      "\n",
      "Epoch 00089: val_loss improved from 3.90826 to 3.84760, saving model to Weights_gs1289.h5\n",
      "\n",
      "Epoch 00090: val_loss improved from 3.84760 to 3.78901, saving model to Weights_gs1290.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 3.78901 to 3.73155, saving model to Weights_gs1291.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 3.73155 to 3.67374, saving model to Weights_gs1292.h5\n",
      "\n",
      "Epoch 00093: val_loss improved from 3.67374 to 3.61916, saving model to Weights_gs1293.h5\n",
      "\n",
      "Epoch 00094: val_loss improved from 3.61916 to 3.56800, saving model to Weights_gs1294.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_loss improved from 3.56800 to 3.51546, saving model to Weights_gs1295.h5\n",
      "\n",
      "Epoch 00096: val_loss improved from 3.51546 to 3.46776, saving model to Weights_gs1296.h5\n",
      "\n",
      "Epoch 00097: val_loss improved from 3.46776 to 3.42138, saving model to Weights_gs1297.h5\n",
      "\n",
      "Epoch 00098: val_loss improved from 3.42138 to 3.37659, saving model to Weights_gs1298.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 3.37659 to 3.33655, saving model to Weights_gs1299.h5\n",
      "\n",
      "Epoch 00100: val_loss improved from 3.33655 to 3.29804, saving model to Weights_gs12100.h5\n",
      "\n",
      "Epoch 00101: val_loss improved from 3.29804 to 3.25916, saving model to Weights_gs12101.h5\n",
      "\n",
      "Epoch 00102: val_loss improved from 3.25916 to 3.22159, saving model to Weights_gs12102.h5\n",
      "\n",
      "Epoch 00103: val_loss improved from 3.22159 to 3.18415, saving model to Weights_gs12103.h5\n",
      "\n",
      "Epoch 00104: val_loss improved from 3.18415 to 3.14771, saving model to Weights_gs12104.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 3.14771 to 3.11372, saving model to Weights_gs12105.h5\n",
      "\n",
      "Epoch 00106: val_loss improved from 3.11372 to 3.08086, saving model to Weights_gs12106.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 3.08086 to 3.04761, saving model to Weights_gs12107.h5\n",
      "\n",
      "Epoch 00108: val_loss improved from 3.04761 to 3.01618, saving model to Weights_gs12108.h5\n",
      "\n",
      "Epoch 00109: val_loss improved from 3.01618 to 2.98651, saving model to Weights_gs12109.h5\n",
      "\n",
      "Epoch 00110: val_loss improved from 2.98651 to 2.95652, saving model to Weights_gs12110.h5\n",
      "\n",
      "Epoch 00111: val_loss improved from 2.95652 to 2.92685, saving model to Weights_gs12111.h5\n",
      "\n",
      "Epoch 00112: val_loss improved from 2.92685 to 2.89889, saving model to Weights_gs12112.h5\n",
      "\n",
      "Epoch 00113: val_loss improved from 2.89889 to 2.87101, saving model to Weights_gs12113.h5\n",
      "\n",
      "Epoch 00114: val_loss improved from 2.87101 to 2.84457, saving model to Weights_gs12114.h5\n",
      "\n",
      "Epoch 00115: val_loss improved from 2.84457 to 2.81975, saving model to Weights_gs12115.h5\n",
      "\n",
      "Epoch 00116: val_loss improved from 2.81975 to 2.79533, saving model to Weights_gs12116.h5\n",
      "\n",
      "Epoch 00117: val_loss improved from 2.79533 to 2.77129, saving model to Weights_gs12117.h5\n",
      "\n",
      "Epoch 00118: val_loss improved from 2.77129 to 2.74691, saving model to Weights_gs12118.h5\n",
      "\n",
      "Epoch 00119: val_loss improved from 2.74691 to 2.72443, saving model to Weights_gs12119.h5\n",
      "\n",
      "Epoch 00120: val_loss improved from 2.72443 to 2.70191, saving model to Weights_gs12120.h5\n",
      "\n",
      "Epoch 00121: val_loss improved from 2.70191 to 2.67957, saving model to Weights_gs12121.h5\n",
      "\n",
      "Epoch 00122: val_loss improved from 2.67957 to 2.65568, saving model to Weights_gs12122.h5\n",
      "\n",
      "Epoch 00123: val_loss improved from 2.65568 to 2.63239, saving model to Weights_gs12123.h5\n",
      "\n",
      "Epoch 00124: val_loss improved from 2.63239 to 2.60964, saving model to Weights_gs12124.h5\n",
      "\n",
      "Epoch 00125: val_loss improved from 2.60964 to 2.58882, saving model to Weights_gs12125.h5\n",
      "\n",
      "Epoch 00126: val_loss improved from 2.58882 to 2.56874, saving model to Weights_gs12126.h5\n",
      "\n",
      "Epoch 00127: val_loss improved from 2.56874 to 2.54677, saving model to Weights_gs12127.h5\n",
      "\n",
      "Epoch 00128: val_loss improved from 2.54677 to 2.52616, saving model to Weights_gs12128.h5\n",
      "\n",
      "Epoch 00129: val_loss improved from 2.52616 to 2.50699, saving model to Weights_gs12129.h5\n",
      "\n",
      "Epoch 00130: val_loss improved from 2.50699 to 2.48745, saving model to Weights_gs12130.h5\n",
      "\n",
      "Epoch 00131: val_loss improved from 2.48745 to 2.46875, saving model to Weights_gs12131.h5\n",
      "\n",
      "Epoch 00132: val_loss improved from 2.46875 to 2.44959, saving model to Weights_gs12132.h5\n",
      "\n",
      "Epoch 00133: val_loss improved from 2.44959 to 2.42944, saving model to Weights_gs12133.h5\n",
      "\n",
      "Epoch 00134: val_loss improved from 2.42944 to 2.40950, saving model to Weights_gs12134.h5\n",
      "\n",
      "Epoch 00135: val_loss improved from 2.40950 to 2.38991, saving model to Weights_gs12135.h5\n",
      "\n",
      "Epoch 00136: val_loss improved from 2.38991 to 2.37093, saving model to Weights_gs12136.h5\n",
      "\n",
      "Epoch 00137: val_loss improved from 2.37093 to 2.35184, saving model to Weights_gs12137.h5\n",
      "\n",
      "Epoch 00138: val_loss improved from 2.35184 to 2.33411, saving model to Weights_gs12138.h5\n",
      "\n",
      "Epoch 00139: val_loss improved from 2.33411 to 2.31589, saving model to Weights_gs12139.h5\n",
      "\n",
      "Epoch 00140: val_loss improved from 2.31589 to 2.29746, saving model to Weights_gs12140.h5\n",
      "\n",
      "Epoch 00141: val_loss improved from 2.29746 to 2.27965, saving model to Weights_gs12141.h5\n",
      "\n",
      "Epoch 00142: val_loss improved from 2.27965 to 2.26144, saving model to Weights_gs12142.h5\n",
      "\n",
      "Epoch 00143: val_loss improved from 2.26144 to 2.24373, saving model to Weights_gs12143.h5\n",
      "\n",
      "Epoch 00144: val_loss improved from 2.24373 to 2.22665, saving model to Weights_gs12144.h5\n",
      "\n",
      "Epoch 00145: val_loss improved from 2.22665 to 2.20892, saving model to Weights_gs12145.h5\n",
      "\n",
      "Epoch 00146: val_loss improved from 2.20892 to 2.19110, saving model to Weights_gs12146.h5\n",
      "\n",
      "Epoch 00147: val_loss improved from 2.19110 to 2.17447, saving model to Weights_gs12147.h5\n",
      "\n",
      "Epoch 00148: val_loss improved from 2.17447 to 2.15767, saving model to Weights_gs12148.h5\n",
      "\n",
      "Epoch 00149: val_loss improved from 2.15767 to 2.14103, saving model to Weights_gs12149.h5\n",
      "\n",
      "Epoch 00150: val_loss improved from 2.14103 to 2.12438, saving model to Weights_gs12150.h5\n",
      "\n",
      "Epoch 00151: val_loss improved from 2.12438 to 2.10847, saving model to Weights_gs12151.h5\n",
      "\n",
      "Epoch 00152: val_loss improved from 2.10847 to 2.09244, saving model to Weights_gs12152.h5\n",
      "\n",
      "Epoch 00153: val_loss improved from 2.09244 to 2.07555, saving model to Weights_gs12153.h5\n",
      "\n",
      "Epoch 00154: val_loss improved from 2.07555 to 2.05966, saving model to Weights_gs12154.h5\n",
      "\n",
      "Epoch 00155: val_loss improved from 2.05966 to 2.04362, saving model to Weights_gs12155.h5\n",
      "\n",
      "Epoch 00156: val_loss improved from 2.04362 to 2.02779, saving model to Weights_gs12156.h5\n",
      "\n",
      "Epoch 00157: val_loss improved from 2.02779 to 2.01280, saving model to Weights_gs12157.h5\n",
      "\n",
      "Epoch 00158: val_loss improved from 2.01280 to 1.99801, saving model to Weights_gs12158.h5\n",
      "\n",
      "Epoch 00159: val_loss improved from 1.99801 to 1.98340, saving model to Weights_gs12159.h5\n",
      "\n",
      "Epoch 00160: val_loss improved from 1.98340 to 1.96858, saving model to Weights_gs12160.h5\n",
      "\n",
      "Epoch 00161: val_loss improved from 1.96858 to 1.95375, saving model to Weights_gs12161.h5\n",
      "\n",
      "Epoch 00162: val_loss improved from 1.95375 to 1.93844, saving model to Weights_gs12162.h5\n",
      "\n",
      "Epoch 00163: val_loss improved from 1.93844 to 1.92405, saving model to Weights_gs12163.h5\n",
      "\n",
      "Epoch 00164: val_loss improved from 1.92405 to 1.90866, saving model to Weights_gs12164.h5\n",
      "\n",
      "Epoch 00165: val_loss improved from 1.90866 to 1.89446, saving model to Weights_gs12165.h5\n",
      "\n",
      "Epoch 00166: val_loss improved from 1.89446 to 1.88031, saving model to Weights_gs12166.h5\n",
      "\n",
      "Epoch 00167: val_loss improved from 1.88031 to 1.86636, saving model to Weights_gs12167.h5\n",
      "\n",
      "Epoch 00168: val_loss improved from 1.86636 to 1.85271, saving model to Weights_gs12168.h5\n",
      "\n",
      "Epoch 00169: val_loss improved from 1.85271 to 1.83925, saving model to Weights_gs12169.h5\n",
      "\n",
      "Epoch 00170: val_loss improved from 1.83925 to 1.82517, saving model to Weights_gs12170.h5\n",
      "\n",
      "Epoch 00171: val_loss improved from 1.82517 to 1.81167, saving model to Weights_gs12171.h5\n",
      "\n",
      "Epoch 00172: val_loss improved from 1.81167 to 1.79886, saving model to Weights_gs12172.h5\n",
      "\n",
      "Epoch 00173: val_loss improved from 1.79886 to 1.78600, saving model to Weights_gs12173.h5\n",
      "\n",
      "Epoch 00174: val_loss improved from 1.78600 to 1.77398, saving model to Weights_gs12174.h5\n",
      "\n",
      "Epoch 00175: val_loss improved from 1.77398 to 1.76161, saving model to Weights_gs12175.h5\n",
      "\n",
      "Epoch 00176: val_loss improved from 1.76161 to 1.74923, saving model to Weights_gs12176.h5\n",
      "\n",
      "Epoch 00177: val_loss improved from 1.74923 to 1.73677, saving model to Weights_gs12177.h5\n",
      "\n",
      "Epoch 00178: val_loss improved from 1.73677 to 1.72377, saving model to Weights_gs12178.h5\n",
      "\n",
      "Epoch 00179: val_loss improved from 1.72377 to 1.71051, saving model to Weights_gs12179.h5\n",
      "\n",
      "Epoch 00180: val_loss improved from 1.71051 to 1.69682, saving model to Weights_gs12180.h5\n",
      "\n",
      "Epoch 00181: val_loss improved from 1.69682 to 1.68385, saving model to Weights_gs12181.h5\n",
      "\n",
      "Epoch 00182: val_loss improved from 1.68385 to 1.67111, saving model to Weights_gs12182.h5\n",
      "\n",
      "Epoch 00183: val_loss improved from 1.67111 to 1.65859, saving model to Weights_gs12183.h5\n",
      "\n",
      "Epoch 00184: val_loss improved from 1.65859 to 1.64564, saving model to Weights_gs12184.h5\n",
      "\n",
      "Epoch 00185: val_loss improved from 1.64564 to 1.63319, saving model to Weights_gs12185.h5\n",
      "\n",
      "Epoch 00186: val_loss improved from 1.63319 to 1.62074, saving model to Weights_gs12186.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00187: val_loss improved from 1.62074 to 1.60817, saving model to Weights_gs12187.h5\n",
      "\n",
      "Epoch 00188: val_loss improved from 1.60817 to 1.59651, saving model to Weights_gs12188.h5\n",
      "\n",
      "Epoch 00189: val_loss improved from 1.59651 to 1.58399, saving model to Weights_gs12189.h5\n",
      "\n",
      "Epoch 00190: val_loss improved from 1.58399 to 1.57235, saving model to Weights_gs12190.h5\n",
      "\n",
      "Epoch 00191: val_loss improved from 1.57235 to 1.56105, saving model to Weights_gs12191.h5\n",
      "\n",
      "Epoch 00192: val_loss improved from 1.56105 to 1.54923, saving model to Weights_gs12192.h5\n",
      "\n",
      "Epoch 00193: val_loss improved from 1.54923 to 1.53665, saving model to Weights_gs12193.h5\n",
      "\n",
      "Epoch 00194: val_loss improved from 1.53665 to 1.52501, saving model to Weights_gs12194.h5\n",
      "\n",
      "Epoch 00195: val_loss improved from 1.52501 to 1.51419, saving model to Weights_gs12195.h5\n",
      "\n",
      "Epoch 00196: val_loss improved from 1.51419 to 1.50292, saving model to Weights_gs12196.h5\n",
      "\n",
      "Epoch 00197: val_loss improved from 1.50292 to 1.49163, saving model to Weights_gs12197.h5\n",
      "\n",
      "Epoch 00198: val_loss improved from 1.49163 to 1.48030, saving model to Weights_gs12198.h5\n",
      "\n",
      "Epoch 00199: val_loss improved from 1.48030 to 1.46940, saving model to Weights_gs12199.h5\n",
      "\n",
      "Epoch 00200: val_loss improved from 1.46940 to 1.45835, saving model to Weights_gs12200.h5\n",
      "\n",
      "Epoch 00201: val_loss improved from 1.45835 to 1.44717, saving model to Weights_gs12201.h5\n",
      "\n",
      "Epoch 00202: val_loss improved from 1.44717 to 1.43655, saving model to Weights_gs12202.h5\n",
      "\n",
      "Epoch 00203: val_loss improved from 1.43655 to 1.42596, saving model to Weights_gs12203.h5\n",
      "\n",
      "Epoch 00204: val_loss improved from 1.42596 to 1.41561, saving model to Weights_gs12204.h5\n",
      "\n",
      "Epoch 00205: val_loss improved from 1.41561 to 1.40536, saving model to Weights_gs12205.h5\n",
      "\n",
      "Epoch 00206: val_loss improved from 1.40536 to 1.39521, saving model to Weights_gs12206.h5\n",
      "\n",
      "Epoch 00207: val_loss improved from 1.39521 to 1.38520, saving model to Weights_gs12207.h5\n",
      "\n",
      "Epoch 00208: val_loss improved from 1.38520 to 1.37527, saving model to Weights_gs12208.h5\n",
      "\n",
      "Epoch 00209: val_loss improved from 1.37527 to 1.36548, saving model to Weights_gs12209.h5\n",
      "\n",
      "Epoch 00210: val_loss improved from 1.36548 to 1.35544, saving model to Weights_gs12210.h5\n",
      "\n",
      "Epoch 00211: val_loss improved from 1.35544 to 1.34539, saving model to Weights_gs12211.h5\n",
      "\n",
      "Epoch 00212: val_loss improved from 1.34539 to 1.33477, saving model to Weights_gs12212.h5\n",
      "\n",
      "Epoch 00213: val_loss improved from 1.33477 to 1.32473, saving model to Weights_gs12213.h5\n",
      "\n",
      "Epoch 00214: val_loss improved from 1.32473 to 1.31512, saving model to Weights_gs12214.h5\n",
      "\n",
      "Epoch 00215: val_loss improved from 1.31512 to 1.30554, saving model to Weights_gs12215.h5\n",
      "\n",
      "Epoch 00216: val_loss improved from 1.30554 to 1.29656, saving model to Weights_gs12216.h5\n",
      "\n",
      "Epoch 00217: val_loss improved from 1.29656 to 1.28687, saving model to Weights_gs12217.h5\n",
      "\n",
      "Epoch 00218: val_loss improved from 1.28687 to 1.27758, saving model to Weights_gs12218.h5\n",
      "\n",
      "Epoch 00219: val_loss improved from 1.27758 to 1.26874, saving model to Weights_gs12219.h5\n",
      "\n",
      "Epoch 00220: val_loss improved from 1.26874 to 1.26026, saving model to Weights_gs12220.h5\n",
      "\n",
      "Epoch 00221: val_loss improved from 1.26026 to 1.25162, saving model to Weights_gs12221.h5\n",
      "\n",
      "Epoch 00222: val_loss improved from 1.25162 to 1.24346, saving model to Weights_gs12222.h5\n",
      "\n",
      "Epoch 00223: val_loss improved from 1.24346 to 1.23469, saving model to Weights_gs12223.h5\n",
      "\n",
      "Epoch 00224: val_loss improved from 1.23469 to 1.22656, saving model to Weights_gs12224.h5\n",
      "\n",
      "Epoch 00225: val_loss improved from 1.22656 to 1.21866, saving model to Weights_gs12225.h5\n",
      "\n",
      "Epoch 00226: val_loss improved from 1.21866 to 1.21011, saving model to Weights_gs12226.h5\n",
      "\n",
      "Epoch 00227: val_loss improved from 1.21011 to 1.20206, saving model to Weights_gs12227.h5\n",
      "\n",
      "Epoch 00228: val_loss improved from 1.20206 to 1.19447, saving model to Weights_gs12228.h5\n",
      "\n",
      "Epoch 00229: val_loss improved from 1.19447 to 1.18673, saving model to Weights_gs12229.h5\n",
      "\n",
      "Epoch 00230: val_loss improved from 1.18673 to 1.17898, saving model to Weights_gs12230.h5\n",
      "\n",
      "Epoch 00231: val_loss improved from 1.17898 to 1.17148, saving model to Weights_gs12231.h5\n",
      "\n",
      "Epoch 00232: val_loss improved from 1.17148 to 1.16475, saving model to Weights_gs12232.h5\n",
      "\n",
      "Epoch 00233: val_loss improved from 1.16475 to 1.15791, saving model to Weights_gs12233.h5\n",
      "\n",
      "Epoch 00234: val_loss improved from 1.15791 to 1.15069, saving model to Weights_gs12234.h5\n",
      "\n",
      "Epoch 00235: val_loss improved from 1.15069 to 1.14359, saving model to Weights_gs12235.h5\n",
      "\n",
      "Epoch 00236: val_loss improved from 1.14359 to 1.13636, saving model to Weights_gs12236.h5\n",
      "\n",
      "Epoch 00237: val_loss improved from 1.13636 to 1.12952, saving model to Weights_gs12237.h5\n",
      "\n",
      "Epoch 00238: val_loss improved from 1.12952 to 1.12258, saving model to Weights_gs12238.h5\n",
      "\n",
      "Epoch 00239: val_loss improved from 1.12258 to 1.11583, saving model to Weights_gs12239.h5\n",
      "\n",
      "Epoch 00240: val_loss improved from 1.11583 to 1.10887, saving model to Weights_gs12240.h5\n",
      "\n",
      "Epoch 00241: val_loss improved from 1.10887 to 1.10249, saving model to Weights_gs12241.h5\n",
      "\n",
      "Epoch 00242: val_loss improved from 1.10249 to 1.09597, saving model to Weights_gs12242.h5\n",
      "\n",
      "Epoch 00243: val_loss improved from 1.09597 to 1.08957, saving model to Weights_gs12243.h5\n",
      "\n",
      "Epoch 00244: val_loss improved from 1.08957 to 1.08290, saving model to Weights_gs12244.h5\n",
      "\n",
      "Epoch 00245: val_loss improved from 1.08290 to 1.07642, saving model to Weights_gs12245.h5\n",
      "\n",
      "Epoch 00246: val_loss improved from 1.07642 to 1.06979, saving model to Weights_gs12246.h5\n",
      "\n",
      "Epoch 00247: val_loss improved from 1.06979 to 1.06385, saving model to Weights_gs12247.h5\n",
      "\n",
      "Epoch 00248: val_loss improved from 1.06385 to 1.05809, saving model to Weights_gs12248.h5\n",
      "\n",
      "Epoch 00249: val_loss improved from 1.05809 to 1.05310, saving model to Weights_gs12249.h5\n",
      "\n",
      "Epoch 00250: val_loss improved from 1.05310 to 1.04766, saving model to Weights_gs12250.h5\n",
      "\n",
      "Epoch 00251: val_loss improved from 1.04766 to 1.04220, saving model to Weights_gs12251.h5\n",
      "\n",
      "Epoch 00252: val_loss improved from 1.04220 to 1.03682, saving model to Weights_gs12252.h5\n",
      "\n",
      "Epoch 00253: val_loss improved from 1.03682 to 1.03189, saving model to Weights_gs12253.h5\n",
      "\n",
      "Epoch 00254: val_loss improved from 1.03189 to 1.02727, saving model to Weights_gs12254.h5\n",
      "\n",
      "Epoch 00255: val_loss improved from 1.02727 to 1.02220, saving model to Weights_gs12255.h5\n",
      "\n",
      "Epoch 00256: val_loss improved from 1.02220 to 1.01766, saving model to Weights_gs12256.h5\n",
      "\n",
      "Epoch 00257: val_loss improved from 1.01766 to 1.01260, saving model to Weights_gs12257.h5\n",
      "\n",
      "Epoch 00258: val_loss improved from 1.01260 to 1.00763, saving model to Weights_gs12258.h5\n",
      "\n",
      "Epoch 00259: val_loss improved from 1.00763 to 1.00294, saving model to Weights_gs12259.h5\n",
      "\n",
      "Epoch 00260: val_loss improved from 1.00294 to 0.99865, saving model to Weights_gs12260.h5\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.99865 to 0.99409, saving model to Weights_gs12261.h5\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.99409 to 0.98943, saving model to Weights_gs12262.h5\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.98943 to 0.98443, saving model to Weights_gs12263.h5\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.98443 to 0.98022, saving model to Weights_gs12264.h5\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.98022 to 0.97646, saving model to Weights_gs12265.h5\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.97646 to 0.97243, saving model to Weights_gs12266.h5\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.97243 to 0.96811, saving model to Weights_gs12267.h5\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.96811 to 0.96442, saving model to Weights_gs12268.h5\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.96442 to 0.96016, saving model to Weights_gs12269.h5\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.96016 to 0.95616, saving model to Weights_gs12270.h5\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.95616 to 0.95179, saving model to Weights_gs12271.h5\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.95179 to 0.94762, saving model to Weights_gs12272.h5\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.94762 to 0.94383, saving model to Weights_gs12273.h5\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.94383 to 0.93997, saving model to Weights_gs12274.h5\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.93997 to 0.93622, saving model to Weights_gs12275.h5\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.93622 to 0.93272, saving model to Weights_gs12276.h5\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.93272 to 0.92922, saving model to Weights_gs12277.h5\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.92922 to 0.92584, saving model to Weights_gs12278.h5\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.92584 to 0.92202, saving model to Weights_gs12279.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00280: val_loss improved from 0.92202 to 0.91870, saving model to Weights_gs12280.h5\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.91870 to 0.91577, saving model to Weights_gs12281.h5\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.91577 to 0.91272, saving model to Weights_gs12282.h5\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.91272 to 0.90970, saving model to Weights_gs12283.h5\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.90970 to 0.90683, saving model to Weights_gs12284.h5\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.90683 to 0.90368, saving model to Weights_gs12285.h5\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.90368 to 0.90101, saving model to Weights_gs12286.h5\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.90101 to 0.89823, saving model to Weights_gs12287.h5\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.89823 to 0.89578, saving model to Weights_gs12288.h5\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.89578 to 0.89313, saving model to Weights_gs12289.h5\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.89313 to 0.89033, saving model to Weights_gs12290.h5\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.89033 to 0.88733, saving model to Weights_gs12291.h5\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.88733 to 0.88419, saving model to Weights_gs12292.h5\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.88419 to 0.88145, saving model to Weights_gs12293.h5\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.88145 to 0.87869, saving model to Weights_gs12294.h5\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.87869 to 0.87627, saving model to Weights_gs12295.h5\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.87627 to 0.87391, saving model to Weights_gs12296.h5\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.87391 to 0.87135, saving model to Weights_gs12297.h5\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.87135 to 0.86879, saving model to Weights_gs12298.h5\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.86879 to 0.86737, saving model to Weights_gs12299.h5\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.86737 to 0.86611, saving model to Weights_gs12300.h5\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.86611 to 0.86427, saving model to Weights_gs12301.h5\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.86427 to 0.86189, saving model to Weights_gs12302.h5\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.86189 to 0.85940, saving model to Weights_gs12303.h5\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.85940 to 0.85731, saving model to Weights_gs12304.h5\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.85731 to 0.85468, saving model to Weights_gs12305.h5\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.85468 to 0.85183, saving model to Weights_gs12306.h5\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.85183 to 0.84933, saving model to Weights_gs12307.h5\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.84933 to 0.84743, saving model to Weights_gs12308.h5\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.84743 to 0.84612, saving model to Weights_gs12309.h5\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.84612 to 0.84421, saving model to Weights_gs12310.h5\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.84421 to 0.84248, saving model to Weights_gs12311.h5\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.84248 to 0.84085, saving model to Weights_gs12312.h5\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.84085 to 0.83886, saving model to Weights_gs12313.h5\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.83886 to 0.83630, saving model to Weights_gs12314.h5\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.83630 to 0.83382, saving model to Weights_gs12315.h5\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.83382 to 0.83223, saving model to Weights_gs12316.h5\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.83223 to 0.83024, saving model to Weights_gs12317.h5\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.83024 to 0.82819, saving model to Weights_gs12318.h5\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.82819 to 0.82599, saving model to Weights_gs12319.h5\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.82599 to 0.82452, saving model to Weights_gs12320.h5\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.82452 to 0.82273, saving model to Weights_gs12321.h5\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.82273 to 0.82074, saving model to Weights_gs12322.h5\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.82074 to 0.81916, saving model to Weights_gs12323.h5\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.81916 to 0.81699, saving model to Weights_gs12324.h5\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.81699 to 0.81521, saving model to Weights_gs12325.h5\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.81521 to 0.81361, saving model to Weights_gs12326.h5\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.81361 to 0.81236, saving model to Weights_gs12327.h5\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.81236 to 0.81107, saving model to Weights_gs12328.h5\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.81107 to 0.80923, saving model to Weights_gs12329.h5\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.80923 to 0.80776, saving model to Weights_gs12330.h5\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.80776 to 0.80629, saving model to Weights_gs12331.h5\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.80629 to 0.80480, saving model to Weights_gs12332.h5\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.80480 to 0.80347, saving model to Weights_gs12333.h5\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.80347 to 0.80175, saving model to Weights_gs12334.h5\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.80175 to 0.80022, saving model to Weights_gs12335.h5\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.80022 to 0.79891, saving model to Weights_gs12336.h5\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.79891 to 0.79796, saving model to Weights_gs12337.h5\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.79796 to 0.79675, saving model to Weights_gs12338.h5\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.79675 to 0.79531, saving model to Weights_gs12339.h5\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.79531 to 0.79329, saving model to Weights_gs12340.h5\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.79329 to 0.79169, saving model to Weights_gs12341.h5\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.79169 to 0.79068, saving model to Weights_gs12342.h5\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.79068 to 0.78969, saving model to Weights_gs12343.h5\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.78969 to 0.78875, saving model to Weights_gs12344.h5\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.78875 to 0.78704, saving model to Weights_gs12345.h5\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.78704 to 0.78587, saving model to Weights_gs12346.h5\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.78587 to 0.78482, saving model to Weights_gs12347.h5\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.78482 to 0.78406, saving model to Weights_gs12348.h5\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.78406 to 0.78293, saving model to Weights_gs12349.h5\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.78293 to 0.78193, saving model to Weights_gs12350.h5\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.78193 to 0.78078, saving model to Weights_gs12351.h5\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.78078 to 0.77951, saving model to Weights_gs12352.h5\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.77951 to 0.77842, saving model to Weights_gs12353.h5\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.77842 to 0.77672, saving model to Weights_gs12354.h5\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.77672 to 0.77494, saving model to Weights_gs12355.h5\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.77494 to 0.77333, saving model to Weights_gs12356.h5\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.77333 to 0.77183, saving model to Weights_gs12357.h5\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.77183 to 0.77022, saving model to Weights_gs12358.h5\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.77022 to 0.76912, saving model to Weights_gs12359.h5\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.76912 to 0.76777, saving model to Weights_gs12360.h5\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.76777 to 0.76691, saving model to Weights_gs12361.h5\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.76691 to 0.76596, saving model to Weights_gs12362.h5\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.76596 to 0.76475, saving model to Weights_gs12363.h5\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.76475 to 0.76338, saving model to Weights_gs12364.h5\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.76338 to 0.76222, saving model to Weights_gs12365.h5\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.76222 to 0.76153, saving model to Weights_gs12366.h5\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.76153 to 0.76075, saving model to Weights_gs12367.h5\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.76075 to 0.75896, saving model to Weights_gs12368.h5\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.75896 to 0.75785, saving model to Weights_gs12369.h5\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.75785 to 0.75627, saving model to Weights_gs12370.h5\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.75627 to 0.75434, saving model to Weights_gs12371.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00372: val_loss improved from 0.75434 to 0.75331, saving model to Weights_gs12372.h5\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.75331 to 0.75208, saving model to Weights_gs12373.h5\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.75208 to 0.75042, saving model to Weights_gs12374.h5\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.75042 to 0.74909, saving model to Weights_gs12375.h5\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.74909 to 0.74813, saving model to Weights_gs12376.h5\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.74813 to 0.74739, saving model to Weights_gs12377.h5\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.74739 to 0.74646, saving model to Weights_gs12378.h5\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.74646 to 0.74600, saving model to Weights_gs12379.h5\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.74600 to 0.74535, saving model to Weights_gs12380.h5\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.74535 to 0.74465, saving model to Weights_gs12381.h5\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.74465 to 0.74367, saving model to Weights_gs12382.h5\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.74367 to 0.74313, saving model to Weights_gs12383.h5\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.74313 to 0.74216, saving model to Weights_gs12384.h5\n",
      "\n",
      "Epoch 00385: val_loss improved from 0.74216 to 0.74138, saving model to Weights_gs12385.h5\n",
      "\n",
      "Epoch 00386: val_loss improved from 0.74138 to 0.74104, saving model to Weights_gs12386.h5\n",
      "\n",
      "Epoch 00387: val_loss improved from 0.74104 to 0.74067, saving model to Weights_gs12387.h5\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.74067 to 0.74015, saving model to Weights_gs12388.h5\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.74015 to 0.73980, saving model to Weights_gs12389.h5\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.73980 to 0.73887, saving model to Weights_gs12390.h5\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.73887 to 0.73796, saving model to Weights_gs12391.h5\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.73796 to 0.73697, saving model to Weights_gs12392.h5\n",
      "\n",
      "Epoch 00393: val_loss improved from 0.73697 to 0.73589, saving model to Weights_gs12393.h5\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.73589 to 0.73532, saving model to Weights_gs12394.h5\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.73532 to 0.73510, saving model to Weights_gs12395.h5\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.73510 to 0.73419, saving model to Weights_gs12396.h5\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.73419 to 0.73304, saving model to Weights_gs12397.h5\n",
      "\n",
      "Epoch 00398: val_loss improved from 0.73304 to 0.73188, saving model to Weights_gs12398.h5\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.73188 to 0.73121, saving model to Weights_gs12399.h5\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.73121 to 0.73035, saving model to Weights_gs12400.h5\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.73035 to 0.72918, saving model to Weights_gs12401.h5\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.72918 to 0.72814, saving model to Weights_gs12402.h5\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.72814 to 0.72750, saving model to Weights_gs12403.h5\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.72750 to 0.72688, saving model to Weights_gs12404.h5\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.72688 to 0.72579, saving model to Weights_gs12405.h5\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.72579 to 0.72459, saving model to Weights_gs12406.h5\n",
      "\n",
      "Epoch 00407: val_loss improved from 0.72459 to 0.72389, saving model to Weights_gs12407.h5\n",
      "\n",
      "Epoch 00408: val_loss improved from 0.72389 to 0.72293, saving model to Weights_gs12408.h5\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.72293 to 0.72219, saving model to Weights_gs12409.h5\n",
      "\n",
      "Epoch 00410: val_loss improved from 0.72219 to 0.72123, saving model to Weights_gs12410.h5\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.72123 to 0.72000, saving model to Weights_gs12411.h5\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.72000 to 0.71940, saving model to Weights_gs12412.h5\n",
      "\n",
      "Epoch 00413: val_loss improved from 0.71940 to 0.71851, saving model to Weights_gs12413.h5\n",
      "\n",
      "Epoch 00414: val_loss improved from 0.71851 to 0.71720, saving model to Weights_gs12414.h5\n",
      "\n",
      "Epoch 00415: val_loss improved from 0.71720 to 0.71597, saving model to Weights_gs12415.h5\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.71597 to 0.71527, saving model to Weights_gs12416.h5\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.71527 to 0.71491, saving model to Weights_gs12417.h5\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.71491 to 0.71356, saving model to Weights_gs12418.h5\n",
      "\n",
      "Epoch 00419: val_loss improved from 0.71356 to 0.71265, saving model to Weights_gs12419.h5\n",
      "\n",
      "Epoch 00420: val_loss improved from 0.71265 to 0.71161, saving model to Weights_gs12420.h5\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.71161 to 0.71097, saving model to Weights_gs12421.h5\n",
      "\n",
      "Epoch 00422: val_loss improved from 0.71097 to 0.71026, saving model to Weights_gs12422.h5\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.71026 to 0.70960, saving model to Weights_gs12423.h5\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.70960 to 0.70867, saving model to Weights_gs12424.h5\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.70867 to 0.70775, saving model to Weights_gs12425.h5\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.70775 to 0.70683, saving model to Weights_gs12426.h5\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.70683 to 0.70623, saving model to Weights_gs12427.h5\n",
      "\n",
      "Epoch 00428: val_loss improved from 0.70623 to 0.70574, saving model to Weights_gs12428.h5\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.70574 to 0.70572, saving model to Weights_gs12429.h5\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.70572\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.70572\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.70572\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.70572 to 0.70410, saving model to Weights_gs12433.h5\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.70410 to 0.70272, saving model to Weights_gs12434.h5\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.70272 to 0.70186, saving model to Weights_gs12435.h5\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.70186 to 0.70094, saving model to Weights_gs12436.h5\n",
      "\n",
      "Epoch 00437: val_loss improved from 0.70094 to 0.70001, saving model to Weights_gs12437.h5\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.70001 to 0.69930, saving model to Weights_gs12438.h5\n",
      "\n",
      "Epoch 00439: val_loss improved from 0.69930 to 0.69871, saving model to Weights_gs12439.h5\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.69871 to 0.69827, saving model to Weights_gs12440.h5\n",
      "\n",
      "Epoch 00441: val_loss improved from 0.69827 to 0.69772, saving model to Weights_gs12441.h5\n",
      "\n",
      "Epoch 00442: val_loss improved from 0.69772 to 0.69750, saving model to Weights_gs12442.h5\n",
      "\n",
      "Epoch 00443: val_loss improved from 0.69750 to 0.69710, saving model to Weights_gs12443.h5\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.69710 to 0.69691, saving model to Weights_gs12444.h5\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.69691\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.69691\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.69691 to 0.69650, saving model to Weights_gs12447.h5\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.69650 to 0.69578, saving model to Weights_gs12448.h5\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.69578 to 0.69520, saving model to Weights_gs12449.h5\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.69520 to 0.69450, saving model to Weights_gs12450.h5\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.69450 to 0.69418, saving model to Weights_gs12451.h5\n",
      "\n",
      "Epoch 00452: val_loss improved from 0.69418 to 0.69369, saving model to Weights_gs12452.h5\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.69369 to 0.69328, saving model to Weights_gs12453.h5\n",
      "\n",
      "Epoch 00454: val_loss improved from 0.69328 to 0.69313, saving model to Weights_gs12454.h5\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.69313 to 0.69263, saving model to Weights_gs12455.h5\n",
      "\n",
      "Epoch 00456: val_loss improved from 0.69263 to 0.69220, saving model to Weights_gs12456.h5\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.69220 to 0.69172, saving model to Weights_gs12457.h5\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.69172 to 0.69141, saving model to Weights_gs12458.h5\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.69141 to 0.69099, saving model to Weights_gs12459.h5\n",
      "\n",
      "Epoch 00460: val_loss improved from 0.69099 to 0.69003, saving model to Weights_gs12460.h5\n",
      "\n",
      "Epoch 00461: val_loss improved from 0.69003 to 0.68930, saving model to Weights_gs12461.h5\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.68930 to 0.68831, saving model to Weights_gs12462.h5\n",
      "\n",
      "Epoch 00463: val_loss improved from 0.68831 to 0.68760, saving model to Weights_gs12463.h5\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.68760 to 0.68706, saving model to Weights_gs12464.h5\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.68706 to 0.68608, saving model to Weights_gs12465.h5\n",
      "\n",
      "Epoch 00466: val_loss improved from 0.68608 to 0.68548, saving model to Weights_gs12466.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00467: val_loss improved from 0.68548 to 0.68517, saving model to Weights_gs12467.h5\n",
      "\n",
      "Epoch 00468: val_loss improved from 0.68517 to 0.68431, saving model to Weights_gs12468.h5\n",
      "\n",
      "Epoch 00469: val_loss improved from 0.68431 to 0.68393, saving model to Weights_gs12469.h5\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.68393 to 0.68353, saving model to Weights_gs12470.h5\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.68353 to 0.68328, saving model to Weights_gs12471.h5\n",
      "\n",
      "Epoch 00472: val_loss improved from 0.68328 to 0.68286, saving model to Weights_gs12472.h5\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.68286 to 0.68234, saving model to Weights_gs12473.h5\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.68234 to 0.68166, saving model to Weights_gs12474.h5\n",
      "\n",
      "Epoch 00475: val_loss improved from 0.68166 to 0.68100, saving model to Weights_gs12475.h5\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.68100 to 0.68090, saving model to Weights_gs12476.h5\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.68090\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.68090\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.68090\n",
      "\n",
      "Epoch 00480: val_loss improved from 0.68090 to 0.68018, saving model to Weights_gs12480.h5\n",
      "\n",
      "Epoch 00481: val_loss improved from 0.68018 to 0.67924, saving model to Weights_gs12481.h5\n",
      "\n",
      "Epoch 00482: val_loss improved from 0.67924 to 0.67829, saving model to Weights_gs12482.h5\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.67829 to 0.67790, saving model to Weights_gs12483.h5\n",
      "\n",
      "Epoch 00484: val_loss improved from 0.67790 to 0.67766, saving model to Weights_gs12484.h5\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.67766\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.67766\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.67766\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.67766\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.67766\n",
      "\n",
      "Epoch 00490: val_loss improved from 0.67766 to 0.67662, saving model to Weights_gs12490.h5\n",
      "\n",
      "Epoch 00491: val_loss improved from 0.67662 to 0.67610, saving model to Weights_gs12491.h5\n",
      "\n",
      "Epoch 00492: val_loss improved from 0.67610 to 0.67579, saving model to Weights_gs12492.h5\n",
      "\n",
      "Epoch 00493: val_loss improved from 0.67579 to 0.67574, saving model to Weights_gs12493.h5\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.67574 to 0.67531, saving model to Weights_gs12494.h5\n",
      "\n",
      "Epoch 00495: val_loss improved from 0.67531 to 0.67507, saving model to Weights_gs12495.h5\n",
      "\n",
      "Epoch 00496: val_loss improved from 0.67507 to 0.67486, saving model to Weights_gs12496.h5\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.67486 to 0.67446, saving model to Weights_gs12497.h5\n",
      "\n",
      "Epoch 00498: val_loss improved from 0.67446 to 0.67409, saving model to Weights_gs12498.h5\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.67409\n",
      "\n",
      "Epoch 00500: val_loss improved from 0.67409 to 0.67395, saving model to Weights_gs12500.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.15075, saving model to Weights_gs1301.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.15075 to 8.56160, saving model to Weights_gs1302.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 8.56160 to 6.86542, saving model to Weights_gs1303.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.86542 to 5.26550, saving model to Weights_gs1304.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.26550 to 4.03266, saving model to Weights_gs1305.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.03266 to 3.33546, saving model to Weights_gs1306.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.33546 to 2.99693, saving model to Weights_gs1307.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.99693 to 2.68131, saving model to Weights_gs1308.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.68131 to 2.38775, saving model to Weights_gs1309.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.38775 to 2.11325, saving model to Weights_gs1310.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.11325 to 1.89092, saving model to Weights_gs1311.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.89092 to 1.70381, saving model to Weights_gs1312.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.70381 to 1.53753, saving model to Weights_gs1313.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.53753 to 1.40029, saving model to Weights_gs1314.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.40029 to 1.27859, saving model to Weights_gs1315.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.27859 to 1.16989, saving model to Weights_gs1316.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.16989 to 1.07449, saving model to Weights_gs1317.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.07449 to 0.99980, saving model to Weights_gs1318.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.99980 to 0.94737, saving model to Weights_gs1319.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.94737 to 0.90546, saving model to Weights_gs1320.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.90546 to 0.87091, saving model to Weights_gs1321.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.87091 to 0.85055, saving model to Weights_gs1322.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.85055 to 0.82139, saving model to Weights_gs1323.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.82139 to 0.80504, saving model to Weights_gs1324.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.80504 to 0.79182, saving model to Weights_gs1325.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.79182 to 0.78623, saving model to Weights_gs1326.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.78623 to 0.76420, saving model to Weights_gs1327.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.76420 to 0.74909, saving model to Weights_gs1328.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.74909 to 0.73636, saving model to Weights_gs1329.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.73636 to 0.72428, saving model to Weights_gs1330.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.72428 to 0.71310, saving model to Weights_gs1331.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.71310 to 0.70826, saving model to Weights_gs1332.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.70826 to 0.69392, saving model to Weights_gs1333.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.69392 to 0.68783, saving model to Weights_gs1334.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.68783 to 0.68372, saving model to Weights_gs1335.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.68372 to 0.67887, saving model to Weights_gs1336.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.67887 to 0.67419, saving model to Weights_gs1337.h5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.67419 to 0.67006, saving model to Weights_gs1338.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.67006\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.67006 to 0.66441, saving model to Weights_gs1340.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.66441 to 0.66308, saving model to Weights_gs1341.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.66308 to 0.65667, saving model to Weights_gs1342.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.65667 to 0.65478, saving model to Weights_gs1343.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.65478 to 0.65116, saving model to Weights_gs1344.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.65116 to 0.64633, saving model to Weights_gs1345.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.64633 to 0.63923, saving model to Weights_gs1346.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.63923 to 0.63612, saving model to Weights_gs1347.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.63612 to 0.63443, saving model to Weights_gs1348.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.63443\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.63443 to 0.63033, saving model to Weights_gs1350.h5\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.63033 to 0.62617, saving model to Weights_gs1351.h5\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.62617 to 0.62373, saving model to Weights_gs1352.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.62373 to 0.61815, saving model to Weights_gs1353.h5\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.61815 to 0.61515, saving model to Weights_gs1354.h5\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.61515 to 0.61136, saving model to Weights_gs1355.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.61136\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.61136 to 0.61031, saving model to Weights_gs1357.h5\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.61031 to 0.60604, saving model to Weights_gs1358.h5\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.60604 to 0.60234, saving model to Weights_gs1359.h5\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.60234 to 0.60219, saving model to Weights_gs1360.h5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.60219\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00063: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.60219\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.60219 to 0.60134, saving model to Weights_gs1372.h5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.60134 to 0.60099, saving model to Weights_gs1373.h5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.60099\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.60099 to 0.60024, saving model to Weights_gs1375.h5\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.60024 to 0.59857, saving model to Weights_gs1376.h5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.59857 to 0.59601, saving model to Weights_gs1377.h5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.59601\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.59601 to 0.58911, saving model to Weights_gs1379.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.58911 to 0.58184, saving model to Weights_gs1380.h5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.58184 to 0.58179, saving model to Weights_gs1381.h5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.58179\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.58179 to 0.57891, saving model to Weights_gs13120.h5\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.57891\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.57891\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.57891\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.57891\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.57891\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.57891\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.57891 to 0.57571, saving model to Weights_gs13127.h5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.57571 to 0.57067, saving model to Weights_gs13128.h5\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.57067\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.57067 to 0.56952, saving model to Weights_gs13130.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.56952\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.56952 to 0.56416, saving model to Weights_gs13141.h5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.56416\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.56416 to 0.56356, saving model to Weights_gs13161.h5\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.56356\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.56356 to 0.56239, saving model to Weights_gs13187.h5\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.56239 to 0.56000, saving model to Weights_gs13188.h5\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.56000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00214: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.56000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00377: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.56000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.91437, saving model to Weights_gs1401.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.91437 to 2.31813, saving model to Weights_gs1402.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.31813 to 1.73501, saving model to Weights_gs1403.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.73501 to 1.50122, saving model to Weights_gs1404.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.50122 to 1.38329, saving model to Weights_gs1405.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.38329 to 1.15709, saving model to Weights_gs1406.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.15709 to 1.02743, saving model to Weights_gs1407.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.02743 to 0.97187, saving model to Weights_gs1408.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.97187 to 0.92140, saving model to Weights_gs1409.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.92140 to 0.88541, saving model to Weights_gs1410.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.88541 to 0.80778, saving model to Weights_gs1411.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.80778\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.80778 to 0.75447, saving model to Weights_gs1413.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.75447\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.75447\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.75447\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.75447\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.75447\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.75447 to 0.71811, saving model to Weights_gs1419.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.71811\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.71811\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.71811\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.71811\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.71811 to 0.69987, saving model to Weights_gs1424.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: val_loss improved from 0.69987 to 0.67205, saving model to Weights_gs1425.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.67205 to 0.66730, saving model to Weights_gs1426.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66730\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66730\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.66730\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66730\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66730\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.66730\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.66730 to 0.66184, saving model to Weights_gs1433.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.66184\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.66184\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66184\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.66184\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.66184\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.66184 to 0.65800, saving model to Weights_gs1439.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.65800\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.65800 to 0.65495, saving model to Weights_gs1471.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.65495\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.65495\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.65495\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.65495\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.65495\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.65495\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.65495\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.65495 to 0.63197, saving model to Weights_gs1479.h5\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.63197\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.63197 to 0.62729, saving model to Weights_gs14104.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.62729 to 0.61996, saving model to Weights_gs14105.h5\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.61996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00181: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.61996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00344: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.61996\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.41718, saving model to Weights_gs1501.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.41718 to 2.34099, saving model to Weights_gs1502.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.34099 to 1.53655, saving model to Weights_gs1503.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.53655 to 1.23069, saving model to Weights_gs1504.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.23069 to 1.04403, saving model to Weights_gs1505.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss improved from 1.04403 to 0.95111, saving model to Weights_gs1506.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.95111 to 0.87975, saving model to Weights_gs1507.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.87975 to 0.81505, saving model to Weights_gs1508.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81505 to 0.78237, saving model to Weights_gs1509.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.78237 to 0.75495, saving model to Weights_gs1510.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.75495 to 0.72427, saving model to Weights_gs1511.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.72427 to 0.70567, saving model to Weights_gs1512.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.70567 to 0.69625, saving model to Weights_gs1513.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.69625 to 0.68850, saving model to Weights_gs1514.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.68850\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.68850 to 0.66676, saving model to Weights_gs1516.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.66676 to 0.63586, saving model to Weights_gs1517.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.63586\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.63586\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.63586 to 0.62645, saving model to Weights_gs1520.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.62645 to 0.59735, saving model to Weights_gs1521.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.59735\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.59735 to 0.59636, saving model to Weights_gs1523.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.59636 to 0.58598, saving model to Weights_gs1524.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.58598\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.58598\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.58598\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.58598\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.58598 to 0.57371, saving model to Weights_gs1529.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.57371\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.57371\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.57371 to 0.55782, saving model to Weights_gs1532.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.55782\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.55782\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.55782\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.55782\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.55782\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.55782\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.55782 to 0.54444, saving model to Weights_gs1539.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.54444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00159: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.54444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00323: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.54444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00485: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.54444\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.54444\n"
     ]
    }
   ],
   "source": [
    "#searching for the best hyperparameter and saving the weights of the best model\n",
    "step_size_list = [0.0001,0.001,1e-1,1e-2]\n",
    "batch_size_list = [16,32,64,128]\n",
    "param_grid = {'step_size': step_size_list,'batch_size': batch_size_list}\n",
    "grid_search = list(ParameterGrid(param_grid))\n",
    "val_loss = []\n",
    "for index,item in enumerate(grid_search):\n",
    "    test_model = my_nn_model(item['step_size'])\n",
    "    weight_file='Weights_gs'+str(index)+'{epoch:02d}.h5'\n",
    "    history = test_model.fit(X_train,y_train,epochs=500,batch_size=item['batch_size'],validation_data = (X_test,y_test),callbacks=model_callbacks(weight_file),verbose=0)\n",
    "    val_loss.append(min(history.history['val_loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51735384859893 1\n",
      "the hyperparameters of the best model {'batch_size': 16, 'step_size': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(min(val_loss),np.argmin(np.array(val_loss)))\n",
    "print(\"the hyperparameters of the best model\",grid_search[np.argmin(np.array(val_loss))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_weights('Weights_gs1118.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared (test): 0.8686955540327765\n",
      "Root mean squared error (test): 0.7180816030369687\n",
      "Mean absolute error (test): 0.5187011401268072\n"
     ]
    }
   ],
   "source": [
    "Y_pred=test_model.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "print('R squared (test):',r2_score(y_test,Y_pred))\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "print('Root mean squared error (test):',np.sqrt(mean_squared_error(y_test,Y_pred)))\n",
    "print('Mean absolute error (test):',mean_absolute_error(y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared (train): 0.9743088772087087\n",
      "Root mean squared error (train): 0.3403829653076838\n",
      "Mean absolute error (train): 0.24099120221265816\n"
     ]
    }
   ],
   "source": [
    "Y_pred2=test_model.predict(X_train)\n",
    "from sklearn.metrics import r2_score\n",
    "print('R squared (train):',r2_score(y_train,Y_pred2))\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "print('Root mean squared error (train):',np.sqrt(mean_squared_error(y_train,Y_pred2)))\n",
    "print('Mean absolute error (train):',mean_absolute_error(y_train,Y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))\n",
    "axes[0].scatter(y_test, Y_pred,label=\"R^2test =\"+str(r2_score(y_test,Y_pred)))\n",
    "axes[0].plot(y_test,y_test,'r--')\n",
    "axes[0].legend()\n",
    "axes[0].set(title = \"test set\",\n",
    "       xlabel = \"predicted values\",\n",
    "       ylabel = \"measured values\")\n",
    "axes[0].grid(True)\n",
    "axes[1].scatter(y_train, Y_pred2)\n",
    "axes[1].plot(y_train,y_train,'r--',label=\"R^2train =\"+str(r2_score(y_train,Y_pred2)))\n",
    "axes[1].set(title = \"train set\",\n",
    "       xlabel = \"predicted values\",\n",
    "       ylabel = \"measured values\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='Training loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.title( 'Loss on training and validation data (Regularisation) ')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting train and validation error vs epoch for the best model\n",
    "model = Sequential([\n",
    "  Dense(len(X_train[0]), activation='relu'),\n",
    "  Dropout(0.2),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dropout(0.2),\n",
    "  Dense(1,activation='linear')])\n",
    "# Compile the model.\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  epochs=120,\n",
    "  batch_size=16,\n",
    "  validation_data = (X_test,y_test),verbose=0\n",
    ")\n",
    "\n",
    "plot_loss(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
